{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b7w2MLmfqaGm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.stats.multicomp as mc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "165TGSKNqkre",
        "outputId": "b2caec33-d390-40d5-c548-cfcf1d5e66ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = pd.read_excel('overalldata (1).xlsx', sheet_name='Sheet1')\n",
        "\n",
        "exclude_models = [\n",
        "    \"codellama-70b-instruct@anyscale\",\n",
        "    \"codellama-7b-instruct@together-ai\",\n",
        "    \"command-r-plus@aws-bedrock\",\n",
        "    \"deepseek-coder-33b-instruct@together-ai\",\n",
        "    \"phi-3-medium-4k-instruct@deepinfra\"\n",
        "]\n",
        "\n",
        "filtered_df = df_new[~df_new['model'].isin(exclude_models)]\n",
        "\n",
        "control_group = filtered_df[filtered_df['model'] == 'original_text']\n",
        "other_groups = filtered_df[filtered_df['model'] != 'original_text']\n",
        "combined_df = pd.concat([control_group, other_groups])\n",
        "\n",
        "model = ols('Q(\"Gunning Fog\") ~ C(model)', data=combined_df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(anova_table)\n",
        "\n",
        "# Tukey's HSD test\n",
        "comp = mc.pairwise_tukeyhsd(combined_df['Gunning Fog'], combined_df['model'])\n",
        "tukey_results = pd.DataFrame(data=comp.summary().data[1:], columns=comp.summary().data[0])\n",
        "\n",
        "control_comparisons = tukey_results[tukey_results['group1'] == 'original_text']\n",
        "\n",
        "# Sort by absolute mean difference to find the most similar models\n",
        "control_comparisons_sorted = control_comparisons.sort_values(by='meandiff', key=abs)\n",
        "print(control_comparisons_sorted.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzchz-z4qpZP",
        "outputId": "e9de8061-27a4-4a93-e8ed-5fc4c0c4a5eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              sum_sq     df         F        PR(>F)\n",
            "C(model)  191.002691   28.0  3.775866  5.577159e-09\n",
            "Residual  495.011669  274.0       NaN           NaN\n",
            "            group1                         group2  meandiff   p-adj  lower  \\\n",
            "405  original_text  qwen-2-72b-instruct@deepinfra    2.2472  0.0034  0.363   \n",
            "\n",
            "      upper  reject  \n",
            "405  4.1315    True  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_new = pd.read_excel('overalldata (1).xlsx', sheet_name='Sheet1')\n",
        "exclude_models = [\n",
        "    \"codellama-70b-instruct@anyscale\",\n",
        "    \"codellama-7b-instruct@together-ai\",\n",
        "    \"command-r-plus@aws-bedrock\",\n",
        "    \"deepseek-coder-33b-instruct@together-ai\",\n",
        "    \"phi-3-medium-4k-instruct@deepinfra\",\n",
        "    \"nemotron-4-340b-instruct@deepinfra\",\n",
        "    \"codellama-13b-instruct@together-ai\"\n",
        "]\n",
        "\n",
        "filtered_df = df_new[~df_new['model'].isin(exclude_models)]\n",
        "summary_table = filtered_df.groupby('model')['Gunning Fog'].describe()\n",
        "\n",
        "print(summary_table)\n",
        "model = ols('Q(\"Gunning Fog\") ~ C(model)', data=filtered_df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(anova_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdjjMQXx33hA",
        "outputId": "befed482-e2f3-4cf9-cc8b-1cb0eeb3c320"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        count       mean       std    min  \\\n",
            "model                                                                       \n",
            "claude-3-haiku@anthropic                 15.0   8.844000  1.453866   5.22   \n",
            "claude-3-opus@anthropic                  15.0   8.386000  1.655263   5.73   \n",
            "claude-3-sonnet@anthropic                 1.0  11.930000       NaN  11.93   \n",
            "claude-3.5-sonnet@anthropic              15.0   7.834667  1.151557   5.39   \n",
            "gemini-1.5-flash@vertex-ai               15.0   7.748667  1.360398   5.44   \n",
            "gemini-1.5-pro@vertex-ai                 15.0   7.212667  0.998559   5.48   \n",
            "gemma-2-9b-it@fireworks-ai               15.0   7.480667  1.341771   5.34   \n",
            "gemma-2b-it@together-ai                  14.0   8.512143  1.530285   5.65   \n",
            "gemma-7b-it@anyscale                      1.0   9.410000       NaN   9.41   \n",
            "gpt-3.5-turbo@openai                     15.0   9.303333  1.234103   7.26   \n",
            "gpt-4-turbo@openai                       15.0   7.874000  1.347319   5.55   \n",
            "gpt-4@openai                             15.0   7.628667  0.943140   6.25   \n",
            "gpt-4o@openai                            15.0   7.324000  1.011666   5.48   \n",
            "llama-3-70b-chat@anyscale                 1.0  11.280000       NaN  11.28   \n",
            "llama-3-70b-chat@fireworks-ai            14.0   9.373571  1.449242   7.33   \n",
            "llama-3-8b-chat@anyscale                  1.0   9.100000       NaN   9.10   \n",
            "llama-3-8b-chat@fireworks-ai             14.0   9.122857  1.795411   5.60   \n",
            "mistral-7b-instruct-v0.1@anyscale         1.0   9.340000       NaN   9.34   \n",
            "mistral-7b-instruct-v0.3@together-ai     15.0   8.987333  1.177840   6.31   \n",
            "mistral-large@aws-bedrock                15.0   8.657333  1.165697   7.01   \n",
            "mistral-small@mistral-ai                 15.0   7.907333  1.120698   6.34   \n",
            "mixtral-8x22b-instruct-v0.1@anyscale      1.0   8.990000       NaN   8.99   \n",
            "mixtral-8x22b-instruct-v0.1@deepinfra    14.0   7.790000  0.969718   6.04   \n",
            "mixtral-8x7b-instruct-v0.1@anyscale       1.0   8.380000       NaN   8.38   \n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock   14.0   8.243571  1.153640   5.81   \n",
            "original_text                            15.0   7.701333  2.304303   4.70   \n",
            "qwen-2-72b-instruct@deepinfra            14.0   9.948571  0.892635   8.15   \n",
            "\n",
            "                                            25%     50%      75%    max  \n",
            "model                                                                    \n",
            "claude-3-haiku@anthropic                 8.1650   8.920  10.0300  10.72  \n",
            "claude-3-opus@anthropic                  7.4150   8.360   9.3000  12.22  \n",
            "claude-3-sonnet@anthropic               11.9300  11.930  11.9300  11.93  \n",
            "claude-3.5-sonnet@anthropic              7.1400   7.660   8.6500   9.75  \n",
            "gemini-1.5-flash@vertex-ai               6.7000   7.750   8.3100  10.22  \n",
            "gemini-1.5-pro@vertex-ai                 6.6500   6.780   7.7900   9.03  \n",
            "gemma-2-9b-it@fireworks-ai               6.3500   7.810   8.2200   9.55  \n",
            "gemma-2b-it@together-ai                  7.0825   8.940   9.7475  10.46  \n",
            "gemma-7b-it@anyscale                     9.4100   9.410   9.4100   9.41  \n",
            "gpt-3.5-turbo@openai                     8.6250   9.250  10.2000  11.25  \n",
            "gpt-4-turbo@openai                       6.9100   8.150   8.8950   9.62  \n",
            "gpt-4@openai                             6.9600   7.540   8.4400   9.11  \n",
            "gpt-4o@openai                            6.5900   7.210   8.3550   8.79  \n",
            "llama-3-70b-chat@anyscale               11.2800  11.280  11.2800  11.28  \n",
            "llama-3-70b-chat@fireworks-ai            8.4375   9.155  10.1975  12.52  \n",
            "llama-3-8b-chat@anyscale                 9.1000   9.100   9.1000   9.10  \n",
            "llama-3-8b-chat@fireworks-ai             7.8200   9.140  10.3125  11.96  \n",
            "mistral-7b-instruct-v0.1@anyscale        9.3400   9.340   9.3400   9.34  \n",
            "mistral-7b-instruct-v0.3@together-ai     8.6400   8.970   9.6600  11.34  \n",
            "mistral-large@aws-bedrock                7.5000   8.800   9.5100  10.36  \n",
            "mistral-small@mistral-ai                 7.1050   7.760   8.6450   9.88  \n",
            "mixtral-8x22b-instruct-v0.1@anyscale     8.9900   8.990   8.9900   8.99  \n",
            "mixtral-8x22b-instruct-v0.1@deepinfra    7.1200   8.160   8.4300   9.03  \n",
            "mixtral-8x7b-instruct-v0.1@anyscale      8.3800   8.380   8.3800   8.38  \n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock   7.7225   8.555   9.1400   9.80  \n",
            "original_text                            6.0550   7.190   9.3750  12.32  \n",
            "qwen-2-72b-instruct@deepinfra            9.4850   9.955  10.6250  11.48  \n",
            "              sum_sq     df         F        PR(>F)\n",
            "C(model)  189.101940   26.0  4.025852  2.280070e-09\n",
            "Residual  495.011669  274.0       NaN           NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = pd.read_excel('overalldata (1).xlsx', sheet_name='Sheet1')\n",
        "exclude_models = [\n",
        "    \"codellama-70b-instruct@anyscale\",\n",
        "    \"codellama-7b-instruct@together-ai\",\n",
        "    \"command-r-plus@aws-bedrock\",\n",
        "    \"deepseek-coder-33b-instruct@together-ai\",\n",
        "    \"phi-3-medium-4k-instruct@deepinfra\",\n",
        "    \"nemotron-4-340b-instruct@deepinfra\",\n",
        "    \"codellama-13b-instruct@together-ai\"\n",
        "]\n",
        "\n",
        "filtered_df = df_new[~df_new['model'].isin(exclude_models)]\n",
        "\n",
        "summary_table = filtered_df.groupby('model')['Gunning Fog'].describe()\n",
        "\n",
        "\n",
        "print(summary_table)\n",
        "model = ols('Q(\"Gunning Fog\") ~ C(model)', data=filtered_df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(anova_table)\n",
        "anova_table.to_excel('anova_table_Gunning Fog.xlsx', index=True)\n",
        "\n",
        "# Tukey's HSD test to compare differences between groups\n",
        "tukey = mc.pairwise_tukeyhsd(filtered_df['Gunning Fog'], filtered_df['model'])\n",
        "\n",
        "print(tukey.summary())\n",
        "tukey_summary_df = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])\n",
        "tukey_summary_df.to_excel('tukey_summary_Gunning Fog.xlsx', index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoEAYIIlQvAZ",
        "outputId": "2559a7f5-5788-4a6f-d102-a28360f8b52c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        count       mean       std    min  \\\n",
            "model                                                                       \n",
            "claude-3-haiku@anthropic                 15.0   8.844000  1.453866   5.22   \n",
            "claude-3-opus@anthropic                  15.0   8.386000  1.655263   5.73   \n",
            "claude-3-sonnet@anthropic                 1.0  11.930000       NaN  11.93   \n",
            "claude-3.5-sonnet@anthropic              15.0   7.834667  1.151557   5.39   \n",
            "gemini-1.5-flash@vertex-ai               15.0   7.748667  1.360398   5.44   \n",
            "gemini-1.5-pro@vertex-ai                 15.0   7.212667  0.998559   5.48   \n",
            "gemma-2-9b-it@fireworks-ai               15.0   7.480667  1.341771   5.34   \n",
            "gemma-2b-it@together-ai                  14.0   8.512143  1.530285   5.65   \n",
            "gemma-7b-it@anyscale                      1.0   9.410000       NaN   9.41   \n",
            "gpt-3.5-turbo@openai                     15.0   9.303333  1.234103   7.26   \n",
            "gpt-4-turbo@openai                       15.0   7.874000  1.347319   5.55   \n",
            "gpt-4@openai                             15.0   7.628667  0.943140   6.25   \n",
            "gpt-4o@openai                            15.0   7.324000  1.011666   5.48   \n",
            "llama-3-70b-chat@anyscale                 1.0  11.280000       NaN  11.28   \n",
            "llama-3-70b-chat@fireworks-ai            14.0   9.373571  1.449242   7.33   \n",
            "llama-3-8b-chat@anyscale                  1.0   9.100000       NaN   9.10   \n",
            "llama-3-8b-chat@fireworks-ai             14.0   9.122857  1.795411   5.60   \n",
            "mistral-7b-instruct-v0.1@anyscale         1.0   9.340000       NaN   9.34   \n",
            "mistral-7b-instruct-v0.3@together-ai     15.0   8.987333  1.177840   6.31   \n",
            "mistral-large@aws-bedrock                15.0   8.657333  1.165697   7.01   \n",
            "mistral-small@mistral-ai                 15.0   7.907333  1.120698   6.34   \n",
            "mixtral-8x22b-instruct-v0.1@anyscale      1.0   8.990000       NaN   8.99   \n",
            "mixtral-8x22b-instruct-v0.1@deepinfra    14.0   7.790000  0.969718   6.04   \n",
            "mixtral-8x7b-instruct-v0.1@anyscale       1.0   8.380000       NaN   8.38   \n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock   14.0   8.243571  1.153640   5.81   \n",
            "original_text                            15.0   7.701333  2.304303   4.70   \n",
            "qwen-2-72b-instruct@deepinfra            14.0   9.948571  0.892635   8.15   \n",
            "\n",
            "                                            25%     50%      75%    max  \n",
            "model                                                                    \n",
            "claude-3-haiku@anthropic                 8.1650   8.920  10.0300  10.72  \n",
            "claude-3-opus@anthropic                  7.4150   8.360   9.3000  12.22  \n",
            "claude-3-sonnet@anthropic               11.9300  11.930  11.9300  11.93  \n",
            "claude-3.5-sonnet@anthropic              7.1400   7.660   8.6500   9.75  \n",
            "gemini-1.5-flash@vertex-ai               6.7000   7.750   8.3100  10.22  \n",
            "gemini-1.5-pro@vertex-ai                 6.6500   6.780   7.7900   9.03  \n",
            "gemma-2-9b-it@fireworks-ai               6.3500   7.810   8.2200   9.55  \n",
            "gemma-2b-it@together-ai                  7.0825   8.940   9.7475  10.46  \n",
            "gemma-7b-it@anyscale                     9.4100   9.410   9.4100   9.41  \n",
            "gpt-3.5-turbo@openai                     8.6250   9.250  10.2000  11.25  \n",
            "gpt-4-turbo@openai                       6.9100   8.150   8.8950   9.62  \n",
            "gpt-4@openai                             6.9600   7.540   8.4400   9.11  \n",
            "gpt-4o@openai                            6.5900   7.210   8.3550   8.79  \n",
            "llama-3-70b-chat@anyscale               11.2800  11.280  11.2800  11.28  \n",
            "llama-3-70b-chat@fireworks-ai            8.4375   9.155  10.1975  12.52  \n",
            "llama-3-8b-chat@anyscale                 9.1000   9.100   9.1000   9.10  \n",
            "llama-3-8b-chat@fireworks-ai             7.8200   9.140  10.3125  11.96  \n",
            "mistral-7b-instruct-v0.1@anyscale        9.3400   9.340   9.3400   9.34  \n",
            "mistral-7b-instruct-v0.3@together-ai     8.6400   8.970   9.6600  11.34  \n",
            "mistral-large@aws-bedrock                7.5000   8.800   9.5100  10.36  \n",
            "mistral-small@mistral-ai                 7.1050   7.760   8.6450   9.88  \n",
            "mixtral-8x22b-instruct-v0.1@anyscale     8.9900   8.990   8.9900   8.99  \n",
            "mixtral-8x22b-instruct-v0.1@deepinfra    7.1200   8.160   8.4300   9.03  \n",
            "mixtral-8x7b-instruct-v0.1@anyscale      8.3800   8.380   8.3800   8.38  \n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock   7.7225   8.555   9.1400   9.80  \n",
            "original_text                            6.0550   7.190   9.3750  12.32  \n",
            "qwen-2-72b-instruct@deepinfra            9.4850   9.955  10.6250  11.48  \n",
            "              sum_sq     df         F        PR(>F)\n",
            "C(model)  189.101940   26.0  4.025852  2.280070e-09\n",
            "Residual  495.011669  274.0       NaN           NaN\n",
            "                                 Multiple Comparison of Means - Tukey HSD, FWER=0.05                                 \n",
            "=====================================================================================================================\n",
            "                group1                                 group2                 meandiff p-adj   lower    upper  reject\n",
            "---------------------------------------------------------------------------------------------------------------------\n",
            "              claude-3-haiku@anthropic                claude-3-opus@anthropic   -0.458    1.0  -2.2915  1.3755  False\n",
            "              claude-3-haiku@anthropic              claude-3-sonnet@anthropic    3.086 0.8932  -2.1001  8.2721  False\n",
            "              claude-3-haiku@anthropic            claude-3.5-sonnet@anthropic  -1.0093 0.9507  -2.8429  0.8242  False\n",
            "              claude-3-haiku@anthropic             gemini-1.5-flash@vertex-ai  -1.0953 0.8894  -2.9289  0.7382  False\n",
            "              claude-3-haiku@anthropic               gemini-1.5-pro@vertex-ai  -1.6313 0.1657  -3.4649  0.2022  False\n",
            "              claude-3-haiku@anthropic             gemma-2-9b-it@fireworks-ai  -1.3633 0.5147  -3.1969  0.4702  False\n",
            "              claude-3-haiku@anthropic                gemma-2b-it@together-ai  -0.3319    1.0  -2.1979  1.5341  False\n",
            "              claude-3-haiku@anthropic                   gemma-7b-it@anyscale    0.566    1.0  -4.6201  5.7521  False\n",
            "              claude-3-haiku@anthropic                   gpt-3.5-turbo@openai   0.4593    1.0  -1.3742  2.2929  False\n",
            "              claude-3-haiku@anthropic                     gpt-4-turbo@openai    -0.97 0.9684  -2.8035  0.8635  False\n",
            "              claude-3-haiku@anthropic                           gpt-4@openai  -1.2153 0.7473  -3.0489  0.6182  False\n",
            "              claude-3-haiku@anthropic                          gpt-4o@openai    -1.52 0.2841  -3.3535  0.3135  False\n",
            "              claude-3-haiku@anthropic              llama-3-70b-chat@anyscale    2.436  0.993  -2.7501  7.6221  False\n",
            "              claude-3-haiku@anthropic          llama-3-70b-chat@fireworks-ai   0.5296    1.0  -1.3364  2.3956  False\n",
            "              claude-3-haiku@anthropic               llama-3-8b-chat@anyscale    0.256    1.0  -4.9301  5.4421  False\n",
            "              claude-3-haiku@anthropic           llama-3-8b-chat@fireworks-ai   0.2789    1.0  -1.5871  2.1449  False\n",
            "              claude-3-haiku@anthropic      mistral-7b-instruct-v0.1@anyscale    0.496    1.0  -4.6901  5.6821  False\n",
            "              claude-3-haiku@anthropic   mistral-7b-instruct-v0.3@together-ai   0.1433    1.0  -1.6902  1.9769  False\n",
            "              claude-3-haiku@anthropic              mistral-large@aws-bedrock  -0.1867    1.0  -2.0202  1.6469  False\n",
            "              claude-3-haiku@anthropic               mistral-small@mistral-ai  -0.9367 0.9792  -2.7702  0.8969  False\n",
            "              claude-3-haiku@anthropic   mixtral-8x22b-instruct-v0.1@anyscale    0.146    1.0  -5.0401  5.3321  False\n",
            "              claude-3-haiku@anthropic  mixtral-8x22b-instruct-v0.1@deepinfra   -1.054 0.9355    -2.92   0.812  False\n",
            "              claude-3-haiku@anthropic    mixtral-8x7b-instruct-v0.1@anyscale   -0.464    1.0  -5.6501  4.7221  False\n",
            "              claude-3-haiku@anthropic mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.6004    1.0  -2.4664  1.2656  False\n",
            "              claude-3-haiku@anthropic                          original_text  -1.1427 0.8408  -2.9762  0.6909  False\n",
            "              claude-3-haiku@anthropic          qwen-2-72b-instruct@deepinfra   1.1046 0.8982  -0.7614  2.9706  False\n",
            "               claude-3-opus@anthropic              claude-3-sonnet@anthropic    3.544 0.6915  -1.6421  8.7301  False\n",
            "               claude-3-opus@anthropic            claude-3.5-sonnet@anthropic  -0.5513    1.0  -2.3849  1.2822  False\n",
            "               claude-3-opus@anthropic             gemini-1.5-flash@vertex-ai  -0.6373 0.9999  -2.4709  1.1962  False\n",
            "               claude-3-opus@anthropic               gemini-1.5-pro@vertex-ai  -1.1733  0.804  -3.0069  0.6602  False\n",
            "               claude-3-opus@anthropic             gemma-2-9b-it@fireworks-ai  -0.9053 0.9864  -2.7389  0.9282  False\n",
            "               claude-3-opus@anthropic                gemma-2b-it@together-ai   0.1261    1.0  -1.7399  1.9921  False\n",
            "               claude-3-opus@anthropic                   gemma-7b-it@anyscale    1.024    1.0  -4.1621  6.2101  False\n",
            "               claude-3-opus@anthropic                   gpt-3.5-turbo@openai   0.9173 0.9839  -0.9162  2.7509  False\n",
            "               claude-3-opus@anthropic                     gpt-4-turbo@openai   -0.512    1.0  -2.3455  1.3215  False\n",
            "               claude-3-opus@anthropic                           gpt-4@openai  -0.7573  0.999  -2.5909  1.0762  False\n",
            "               claude-3-opus@anthropic                          gpt-4o@openai   -1.062 0.9172  -2.8955  0.7715  False\n",
            "               claude-3-opus@anthropic              llama-3-70b-chat@anyscale    2.894 0.9431  -2.2921  8.0801  False\n",
            "               claude-3-opus@anthropic          llama-3-70b-chat@fireworks-ai   0.9876 0.9682  -0.8784  2.8536  False\n",
            "               claude-3-opus@anthropic               llama-3-8b-chat@anyscale    0.714    1.0  -4.4721  5.9001  False\n",
            "               claude-3-opus@anthropic           llama-3-8b-chat@fireworks-ai   0.7369 0.9995  -1.1291  2.6029  False\n",
            "               claude-3-opus@anthropic      mistral-7b-instruct-v0.1@anyscale    0.954    1.0  -4.2321  6.1401  False\n",
            "               claude-3-opus@anthropic   mistral-7b-instruct-v0.3@together-ai   0.6013    1.0  -1.2322  2.4349  False\n",
            "               claude-3-opus@anthropic              mistral-large@aws-bedrock   0.2713    1.0  -1.5622  2.1049  False\n",
            "               claude-3-opus@anthropic               mistral-small@mistral-ai  -0.4787    1.0  -2.3122  1.3549  False\n",
            "               claude-3-opus@anthropic   mixtral-8x22b-instruct-v0.1@anyscale    0.604    1.0  -4.5821  5.7901  False\n",
            "               claude-3-opus@anthropic  mixtral-8x22b-instruct-v0.1@deepinfra   -0.596    1.0   -2.462    1.27  False\n",
            "               claude-3-opus@anthropic    mixtral-8x7b-instruct-v0.1@anyscale   -0.006    1.0  -5.1921  5.1801  False\n",
            "               claude-3-opus@anthropic mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.1424    1.0  -2.0084  1.7236  False\n",
            "               claude-3-opus@anthropic                          original_text  -0.6847 0.9998  -2.5182  1.1489  False\n",
            "               claude-3-opus@anthropic          qwen-2-72b-instruct@deepinfra   1.5626 0.2652  -0.3034  3.4286  False\n",
            "             claude-3-sonnet@anthropic            claude-3.5-sonnet@anthropic  -4.0953 0.3829  -9.2814  1.0907  False\n",
            "             claude-3-sonnet@anthropic             gemini-1.5-flash@vertex-ai  -4.1813 0.3393  -9.3674  1.0047  False\n",
            "             claude-3-sonnet@anthropic               gemini-1.5-pro@vertex-ai  -4.7173 0.1362  -9.9034  0.4687  False\n",
            "             claude-3-sonnet@anthropic             gemma-2-9b-it@fireworks-ai  -4.4493 0.2223  -9.6354  0.7367  False\n",
            "             claude-3-sonnet@anthropic                gemma-2b-it@together-ai  -3.4179 0.7609  -8.6155  1.7798  False\n",
            "             claude-3-sonnet@anthropic                   gemma-7b-it@anyscale    -2.52 0.9999  -9.6213  4.5813  False\n",
            "             claude-3-sonnet@anthropic                   gpt-3.5-turbo@openai  -2.6267 0.9812  -7.8127  2.5594  False\n",
            "             claude-3-sonnet@anthropic                     gpt-4-turbo@openai   -4.056 0.4036  -9.2421  1.1301  False\n",
            "             claude-3-sonnet@anthropic                           gpt-4@openai  -4.3013 0.2832  -9.4874  0.8847  False\n",
            "             claude-3-sonnet@anthropic                          gpt-4o@openai   -4.606 0.1682  -9.7921  0.5801  False\n",
            "             claude-3-sonnet@anthropic              llama-3-70b-chat@anyscale    -0.65    1.0  -7.7513  6.4513  False\n",
            "             claude-3-sonnet@anthropic          llama-3-70b-chat@fireworks-ai  -2.5564 0.9871   -7.754  2.6412  False\n",
            "             claude-3-sonnet@anthropic               llama-3-8b-chat@anyscale    -2.83 0.9994  -9.9313  4.2713  False\n",
            "             claude-3-sonnet@anthropic           llama-3-8b-chat@fireworks-ai  -2.8071   0.96  -8.0048  2.3905  False\n",
            "             claude-3-sonnet@anthropic      mistral-7b-instruct-v0.1@anyscale    -2.59 0.9999  -9.6913  4.5113  False\n",
            "             claude-3-sonnet@anthropic   mistral-7b-instruct-v0.3@together-ai  -2.9427 0.9324  -8.1287  2.2434  False\n",
            "             claude-3-sonnet@anthropic              mistral-large@aws-bedrock  -3.2727  0.824  -8.4587  1.9134  False\n",
            "             claude-3-sonnet@anthropic               mistral-small@mistral-ai  -4.0227 0.4215  -9.2087  1.1634  False\n",
            "             claude-3-sonnet@anthropic   mixtral-8x22b-instruct-v0.1@anyscale    -2.94 0.9989 -10.0413  4.1613  False\n",
            "             claude-3-sonnet@anthropic  mixtral-8x22b-instruct-v0.1@deepinfra    -4.14 0.3646  -9.3376  1.0576  False\n",
            "             claude-3-sonnet@anthropic    mixtral-8x7b-instruct-v0.1@anyscale    -3.55 0.9841 -10.6513  3.5513  False\n",
            "             claude-3-sonnet@anthropic mixtral-8x7b-instruct-v0.1@aws-bedrock  -3.6864 0.6168   -8.884  1.5112  False\n",
            "             claude-3-sonnet@anthropic                          original_text  -4.2287 0.3165  -9.4147  0.9574  False\n",
            "             claude-3-sonnet@anthropic          qwen-2-72b-instruct@deepinfra  -1.9814 0.9997   -7.179  3.2162  False\n",
            "           claude-3.5-sonnet@anthropic             gemini-1.5-flash@vertex-ai   -0.086    1.0  -1.9195  1.7475  False\n",
            "           claude-3.5-sonnet@anthropic               gemini-1.5-pro@vertex-ai   -0.622    1.0  -2.4555  1.2115  False\n",
            "           claude-3.5-sonnet@anthropic             gemma-2-9b-it@fireworks-ai   -0.354    1.0  -2.1875  1.4795  False\n",
            "           claude-3.5-sonnet@anthropic                gemma-2b-it@together-ai   0.6775 0.9999  -1.1885  2.5435  False\n",
            "           claude-3.5-sonnet@anthropic                   gemma-7b-it@anyscale   1.5753    1.0  -3.6107  6.7614  False\n",
            "           claude-3.5-sonnet@anthropic                   gpt-3.5-turbo@openai   1.4687 0.3529  -0.3649  3.3022  False\n",
            "           claude-3.5-sonnet@anthropic                     gpt-4-turbo@openai   0.0393    1.0  -1.7942  1.8729  False\n",
            "           claude-3.5-sonnet@anthropic                           gpt-4@openai   -0.206    1.0  -2.0395  1.6275  False\n",
            "           claude-3.5-sonnet@anthropic                          gpt-4o@openai  -0.5107    1.0  -2.3442  1.3229  False\n",
            "           claude-3.5-sonnet@anthropic              llama-3-70b-chat@anyscale   3.4453 0.7433  -1.7407  8.6314  False\n",
            "           claude-3.5-sonnet@anthropic          llama-3-70b-chat@fireworks-ai   1.5389 0.2941  -0.3271  3.4049  False\n",
            "           claude-3.5-sonnet@anthropic               llama-3-8b-chat@anyscale   1.2653    1.0  -3.9207  6.4514  False\n",
            "           claude-3.5-sonnet@anthropic           llama-3-8b-chat@fireworks-ai   1.2882 0.6718  -0.5778  3.1542  False\n",
            "           claude-3.5-sonnet@anthropic      mistral-7b-instruct-v0.1@anyscale   1.5053    1.0  -3.6807  6.6914  False\n",
            "           claude-3.5-sonnet@anthropic   mistral-7b-instruct-v0.3@together-ai   1.1527 0.8293  -0.6809  2.9862  False\n",
            "           claude-3.5-sonnet@anthropic              mistral-large@aws-bedrock   0.8227 0.9964  -1.0109  2.6562  False\n",
            "           claude-3.5-sonnet@anthropic               mistral-small@mistral-ai   0.0727    1.0  -1.7609  1.9062  False\n",
            "           claude-3.5-sonnet@anthropic   mixtral-8x22b-instruct-v0.1@anyscale   1.1553    1.0  -4.0307  6.3414  False\n",
            "           claude-3.5-sonnet@anthropic  mixtral-8x22b-instruct-v0.1@deepinfra  -0.0447    1.0  -1.9107  1.8213  False\n",
            "           claude-3.5-sonnet@anthropic    mixtral-8x7b-instruct-v0.1@anyscale   0.5453    1.0  -4.6407  5.7314  False\n",
            "           claude-3.5-sonnet@anthropic mixtral-8x7b-instruct-v0.1@aws-bedrock   0.4089    1.0  -1.4571  2.2749  False\n",
            "           claude-3.5-sonnet@anthropic                          original_text  -0.1333    1.0  -1.9669  1.7002  False\n",
            "           claude-3.5-sonnet@anthropic          qwen-2-72b-instruct@deepinfra   2.1139 0.0086   0.2479  3.9799   True\n",
            "            gemini-1.5-flash@vertex-ai               gemini-1.5-pro@vertex-ai   -0.536    1.0  -2.3695  1.2975  False\n",
            "            gemini-1.5-flash@vertex-ai             gemma-2-9b-it@fireworks-ai   -0.268    1.0  -2.1015  1.5655  False\n",
            "            gemini-1.5-flash@vertex-ai                gemma-2b-it@together-ai   0.7635 0.9991  -1.1025  2.6295  False\n",
            "            gemini-1.5-flash@vertex-ai                   gemma-7b-it@anyscale   1.6613    1.0  -3.5247  6.8474  False\n",
            "            gemini-1.5-flash@vertex-ai                   gpt-3.5-turbo@openai   1.5547 0.2426  -0.2789  3.3882  False\n",
            "            gemini-1.5-flash@vertex-ai                     gpt-4-turbo@openai   0.1253    1.0  -1.7082  1.9589  False\n",
            "            gemini-1.5-flash@vertex-ai                           gpt-4@openai    -0.12    1.0  -1.9535  1.7135  False\n",
            "            gemini-1.5-flash@vertex-ai                          gpt-4o@openai  -0.4247    1.0  -2.2582  1.4089  False\n",
            "            gemini-1.5-flash@vertex-ai              llama-3-70b-chat@anyscale   3.5313 0.6984  -1.6547  8.7174  False\n",
            "            gemini-1.5-flash@vertex-ai          llama-3-70b-chat@fireworks-ai   1.6249  0.198  -0.2411  3.4909  False\n",
            "            gemini-1.5-flash@vertex-ai               llama-3-8b-chat@anyscale   1.3513    1.0  -3.8347  6.5374  False\n",
            "            gemini-1.5-flash@vertex-ai           llama-3-8b-chat@fireworks-ai   1.3742 0.5358  -0.4918  3.2402  False\n",
            "            gemini-1.5-flash@vertex-ai      mistral-7b-instruct-v0.1@anyscale   1.5913    1.0  -3.5947  6.7774  False\n",
            "            gemini-1.5-flash@vertex-ai   mistral-7b-instruct-v0.3@together-ai   1.2387 0.7132  -0.5949  3.0722  False\n",
            "            gemini-1.5-flash@vertex-ai              mistral-large@aws-bedrock   0.9087 0.9858  -0.9249  2.7422  False\n",
            "            gemini-1.5-flash@vertex-ai               mistral-small@mistral-ai   0.1587    1.0  -1.6749  1.9922  False\n",
            "            gemini-1.5-flash@vertex-ai   mixtral-8x22b-instruct-v0.1@anyscale   1.2413    1.0  -3.9447  6.4274  False\n",
            "            gemini-1.5-flash@vertex-ai  mixtral-8x22b-instruct-v0.1@deepinfra   0.0413    1.0  -1.8247  1.9073  False\n",
            "            gemini-1.5-flash@vertex-ai    mixtral-8x7b-instruct-v0.1@anyscale   0.6313    1.0  -4.5547  5.8174  False\n",
            "            gemini-1.5-flash@vertex-ai mixtral-8x7b-instruct-v0.1@aws-bedrock   0.4949    1.0  -1.3711  2.3609  False\n",
            "            gemini-1.5-flash@vertex-ai                          original_text  -0.0473    1.0  -1.8809  1.7862  False\n",
            "            gemini-1.5-flash@vertex-ai          qwen-2-72b-instruct@deepinfra   2.1999 0.0044   0.3339  4.0659   True\n",
            "              gemini-1.5-pro@vertex-ai             gemma-2-9b-it@fireworks-ai    0.268    1.0  -1.5655  2.1015  False\n",
            "              gemini-1.5-pro@vertex-ai                gemma-2b-it@together-ai   1.2995 0.6544  -0.5665  3.1655  False\n",
            "              gemini-1.5-pro@vertex-ai                   gemma-7b-it@anyscale   2.1973 0.9985  -2.9887  7.3834  False\n",
            "              gemini-1.5-pro@vertex-ai                   gpt-3.5-turbo@openai   2.0907 0.0077   0.2571  3.9242   True\n",
            "              gemini-1.5-pro@vertex-ai                     gpt-4-turbo@openai   0.6613 0.9999  -1.1722  2.4949  False\n",
            "              gemini-1.5-pro@vertex-ai                           gpt-4@openai    0.416    1.0  -1.4175  2.2495  False\n",
            "              gemini-1.5-pro@vertex-ai                          gpt-4o@openai   0.1113    1.0  -1.7222  1.9449  False\n",
            "              gemini-1.5-pro@vertex-ai              llama-3-70b-chat@anyscale   4.0673 0.3976  -1.1187  9.2534  False\n",
            "              gemini-1.5-pro@vertex-ai          llama-3-70b-chat@fireworks-ai   2.1609  0.006   0.2949  4.0269   True\n",
            "              gemini-1.5-pro@vertex-ai               llama-3-8b-chat@anyscale   1.8873 0.9999  -3.2987  7.0734  False\n",
            "              gemini-1.5-pro@vertex-ai           llama-3-8b-chat@fireworks-ai   1.9102 0.0374   0.0442  3.7762   True\n",
            "              gemini-1.5-pro@vertex-ai      mistral-7b-instruct-v0.1@anyscale   2.1273 0.9991  -3.0587  7.3134  False\n",
            "              gemini-1.5-pro@vertex-ai   mistral-7b-instruct-v0.3@together-ai   1.7747 0.0728  -0.0589  3.6082  False\n",
            "              gemini-1.5-pro@vertex-ai              mistral-large@aws-bedrock   1.4447 0.3877  -0.3889  3.2782  False\n",
            "              gemini-1.5-pro@vertex-ai               mistral-small@mistral-ai   0.6947 0.9998  -1.1389  2.5282  False\n",
            "              gemini-1.5-pro@vertex-ai   mixtral-8x22b-instruct-v0.1@anyscale   1.7773    1.0  -3.4087  6.9634  False\n",
            "              gemini-1.5-pro@vertex-ai  mixtral-8x22b-instruct-v0.1@deepinfra   0.5773    1.0  -1.2887  2.4433  False\n",
            "              gemini-1.5-pro@vertex-ai    mixtral-8x7b-instruct-v0.1@anyscale   1.1673    1.0  -4.0187  6.3534  False\n",
            "              gemini-1.5-pro@vertex-ai mixtral-8x7b-instruct-v0.1@aws-bedrock   1.0309 0.9488  -0.8351  2.8969  False\n",
            "              gemini-1.5-pro@vertex-ai                          original_text   0.4887    1.0  -1.3449  2.3222  False\n",
            "              gemini-1.5-pro@vertex-ai          qwen-2-72b-instruct@deepinfra   2.7359    0.0   0.8699  4.6019   True\n",
            "            gemma-2-9b-it@fireworks-ai                gemma-2b-it@together-ai   1.0315 0.9485  -0.8345  2.8975  False\n",
            "            gemma-2-9b-it@fireworks-ai                   gemma-7b-it@anyscale   1.9293 0.9998  -3.2567  7.1154  False\n",
            "            gemma-2-9b-it@fireworks-ai                   gpt-3.5-turbo@openai   1.8227 0.0537  -0.0109  3.6562  False\n",
            "            gemma-2-9b-it@fireworks-ai                     gpt-4-turbo@openai   0.3933    1.0  -1.4402  2.2269  False\n",
            "            gemma-2-9b-it@fireworks-ai                           gpt-4@openai    0.148    1.0  -1.6855  1.9815  False\n",
            "            gemma-2-9b-it@fireworks-ai                          gpt-4o@openai  -0.1567    1.0  -1.9902  1.6769  False\n",
            "            gemma-2-9b-it@fireworks-ai              llama-3-70b-chat@anyscale   3.7993 0.5473  -1.3867  8.9854  False\n",
            "            gemma-2-9b-it@fireworks-ai          llama-3-70b-chat@fireworks-ai   1.8929  0.042   0.0269  3.7589   True\n",
            "            gemma-2-9b-it@fireworks-ai               llama-3-8b-chat@anyscale   1.6193    1.0  -3.5667  6.8054  False\n",
            "            gemma-2-9b-it@fireworks-ai           llama-3-8b-chat@fireworks-ai   1.6422 0.1816  -0.2238  3.5082  False\n",
            "            gemma-2-9b-it@fireworks-ai      mistral-7b-instruct-v0.1@anyscale   1.8593 0.9999  -3.3267  7.0454  False\n",
            "            gemma-2-9b-it@fireworks-ai   mistral-7b-instruct-v0.3@together-ai   1.5067 0.3012  -0.3269  3.3402  False\n",
            "            gemma-2-9b-it@fireworks-ai              mistral-large@aws-bedrock   1.1767 0.7997  -0.6569  3.0102  False\n",
            "            gemma-2-9b-it@fireworks-ai               mistral-small@mistral-ai   0.4267    1.0  -1.4069  2.2602  False\n",
            "            gemma-2-9b-it@fireworks-ai   mixtral-8x22b-instruct-v0.1@anyscale   1.5093    1.0  -3.6767  6.6954  False\n",
            "            gemma-2-9b-it@fireworks-ai  mixtral-8x22b-instruct-v0.1@deepinfra   0.3093    1.0  -1.5567  2.1753  False\n",
            "            gemma-2-9b-it@fireworks-ai    mixtral-8x7b-instruct-v0.1@anyscale   0.8993    1.0  -4.2867  6.0854  False\n",
            "            gemma-2-9b-it@fireworks-ai mixtral-8x7b-instruct-v0.1@aws-bedrock   0.7629 0.9991  -1.1031  2.6289  False\n",
            "            gemma-2-9b-it@fireworks-ai                          original_text   0.2207    1.0  -1.6129  2.0542  False\n",
            "            gemma-2-9b-it@fireworks-ai          qwen-2-72b-instruct@deepinfra   2.4679 0.0004   0.6019  4.3339   True\n",
            "               gemma-2b-it@together-ai                   gemma-7b-it@anyscale   0.8979    1.0  -4.2998  6.0955  False\n",
            "               gemma-2b-it@together-ai                   gpt-3.5-turbo@openai   0.7912 0.9985  -1.0748  2.6572  False\n",
            "               gemma-2b-it@together-ai                     gpt-4-turbo@openai  -0.6381    1.0  -2.5041  1.2279  False\n",
            "               gemma-2b-it@together-ai                           gpt-4@openai  -0.8835 0.9922  -2.7495  0.9825  False\n",
            "               gemma-2b-it@together-ai                          gpt-4o@openai  -1.1881 0.8113  -3.0541  0.6779  False\n",
            "               gemma-2b-it@together-ai              llama-3-70b-chat@anyscale   2.7679 0.9659  -2.4298  7.9655  False\n",
            "               gemma-2b-it@together-ai          llama-3-70b-chat@fireworks-ai   0.8614 0.9957  -1.0365  2.7593  False\n",
            "               gemma-2b-it@together-ai               llama-3-8b-chat@anyscale   0.5879    1.0  -4.6098  5.7855  False\n",
            "               gemma-2b-it@together-ai           llama-3-8b-chat@fireworks-ai   0.6107    1.0  -1.2872  2.5086  False\n",
            "               gemma-2b-it@together-ai      mistral-7b-instruct-v0.1@anyscale   0.8279    1.0  -4.3698  6.0255  False\n",
            "               gemma-2b-it@together-ai   mistral-7b-instruct-v0.3@together-ai   0.4752    1.0  -1.3908  2.3412  False\n",
            "               gemma-2b-it@together-ai              mistral-large@aws-bedrock   0.1452    1.0  -1.7208  2.0112  False\n",
            "               gemma-2b-it@together-ai               mistral-small@mistral-ai  -0.6048    1.0  -2.4708  1.2612  False\n",
            "               gemma-2b-it@together-ai   mixtral-8x22b-instruct-v0.1@anyscale   0.4779    1.0  -4.7198  5.6755  False\n",
            "               gemma-2b-it@together-ai  mixtral-8x22b-instruct-v0.1@deepinfra  -0.7221 0.9997    -2.62  1.1758  False\n",
            "               gemma-2b-it@together-ai    mixtral-8x7b-instruct-v0.1@anyscale  -0.1321    1.0  -5.3298  5.0655  False\n",
            "               gemma-2b-it@together-ai mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.2686    1.0  -2.1665  1.6293  False\n",
            "               gemma-2b-it@together-ai                          original_text  -0.8108 0.9978  -2.6768  1.0552  False\n",
            "               gemma-2b-it@together-ai          qwen-2-72b-instruct@deepinfra   1.4364 0.4755  -0.4615  3.3343  False\n",
            "                  gemma-7b-it@anyscale                   gpt-3.5-turbo@openai  -0.1067    1.0  -5.2927  5.0794  False\n",
            "                  gemma-7b-it@anyscale                     gpt-4-turbo@openai   -1.536    1.0  -6.7221  3.6501  False\n",
            "                  gemma-7b-it@anyscale                           gpt-4@openai  -1.7813    1.0  -6.9674  3.4047  False\n",
            "                  gemma-7b-it@anyscale                          gpt-4o@openai   -2.086 0.9993  -7.2721  3.1001  False\n",
            "                  gemma-7b-it@anyscale              llama-3-70b-chat@anyscale     1.87    1.0  -5.2313  8.9713  False\n",
            "                  gemma-7b-it@anyscale          llama-3-70b-chat@fireworks-ai  -0.0364    1.0   -5.234  5.1612  False\n",
            "                  gemma-7b-it@anyscale               llama-3-8b-chat@anyscale    -0.31    1.0  -7.4113  6.7913  False\n",
            "                  gemma-7b-it@anyscale           llama-3-8b-chat@fireworks-ai  -0.2871    1.0  -5.4848  4.9105  False\n",
            "                  gemma-7b-it@anyscale      mistral-7b-instruct-v0.1@anyscale    -0.07    1.0  -7.1713  7.0313  False\n",
            "                  gemma-7b-it@anyscale   mistral-7b-instruct-v0.3@together-ai  -0.4227    1.0  -5.6087  4.7634  False\n",
            "                  gemma-7b-it@anyscale              mistral-large@aws-bedrock  -0.7527    1.0  -5.9387  4.4334  False\n",
            "                  gemma-7b-it@anyscale               mistral-small@mistral-ai  -1.5027    1.0  -6.6887  3.6834  False\n",
            "                  gemma-7b-it@anyscale   mixtral-8x22b-instruct-v0.1@anyscale    -0.42    1.0  -7.5213  6.6813  False\n",
            "                  gemma-7b-it@anyscale  mixtral-8x22b-instruct-v0.1@deepinfra    -1.62    1.0  -6.8176  3.5776  False\n",
            "                  gemma-7b-it@anyscale    mixtral-8x7b-instruct-v0.1@anyscale    -1.03    1.0  -8.1313  6.0713  False\n",
            "                  gemma-7b-it@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.1664    1.0   -6.364  4.0312  False\n",
            "                  gemma-7b-it@anyscale                          original_text  -1.7087    1.0  -6.8947  3.4774  False\n",
            "                  gemma-7b-it@anyscale          qwen-2-72b-instruct@deepinfra   0.5386    1.0   -4.659  5.7362  False\n",
            "                  gpt-3.5-turbo@openai                     gpt-4-turbo@openai  -1.4293 0.4107  -3.2629  0.4042  False\n",
            "                  gpt-3.5-turbo@openai                           gpt-4@openai  -1.6747 0.1311  -3.5082  0.1589  False\n",
            "                  gpt-3.5-turbo@openai                          gpt-4o@openai  -1.9793 0.0181  -3.8129 -0.1458   True\n",
            "                  gpt-3.5-turbo@openai              llama-3-70b-chat@anyscale   1.9767 0.9997  -3.2094  7.1627  False\n",
            "                  gpt-3.5-turbo@openai          llama-3-70b-chat@fireworks-ai   0.0702    1.0  -1.7958  1.9362  False\n",
            "                  gpt-3.5-turbo@openai               llama-3-8b-chat@anyscale  -0.2033    1.0  -5.3894  4.9827  False\n",
            "                  gpt-3.5-turbo@openai           llama-3-8b-chat@fireworks-ai  -0.1805    1.0  -2.0465  1.6855  False\n",
            "                  gpt-3.5-turbo@openai      mistral-7b-instruct-v0.1@anyscale   0.0367    1.0  -5.1494  5.2227  False\n",
            "                  gpt-3.5-turbo@openai   mistral-7b-instruct-v0.3@together-ai   -0.316    1.0  -2.1495  1.5175  False\n",
            "                  gpt-3.5-turbo@openai              mistral-large@aws-bedrock   -0.646 0.9999  -2.4795  1.1875  False\n",
            "                  gpt-3.5-turbo@openai               mistral-small@mistral-ai   -1.396 0.4624  -3.2295  0.4375  False\n",
            "                  gpt-3.5-turbo@openai   mixtral-8x22b-instruct-v0.1@anyscale  -0.3133    1.0  -5.4994  4.8727  False\n",
            "                  gpt-3.5-turbo@openai  mixtral-8x22b-instruct-v0.1@deepinfra  -1.5133 0.3274  -3.3793  0.3527  False\n",
            "                  gpt-3.5-turbo@openai    mixtral-8x7b-instruct-v0.1@anyscale  -0.9233    1.0  -6.1094  4.2627  False\n",
            "                  gpt-3.5-turbo@openai mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.0598 0.9318  -2.9258  0.8062  False\n",
            "                  gpt-3.5-turbo@openai                          original_text   -1.602 0.1927  -3.4355  0.2315  False\n",
            "                  gpt-3.5-turbo@openai          qwen-2-72b-instruct@deepinfra   0.6452    1.0  -1.2208  2.5112  False\n",
            "                    gpt-4-turbo@openai                           gpt-4@openai  -0.2453    1.0  -2.0789  1.5882  False\n",
            "                    gpt-4-turbo@openai                          gpt-4o@openai    -0.55    1.0  -2.3835  1.2835  False\n",
            "                    gpt-4-turbo@openai              llama-3-70b-chat@anyscale    3.406 0.7629  -1.7801  8.5921  False\n",
            "                    gpt-4-turbo@openai          llama-3-70b-chat@fireworks-ai   1.4996 0.3461  -0.3664  3.3656  False\n",
            "                    gpt-4-turbo@openai               llama-3-8b-chat@anyscale    1.226    1.0  -3.9601  6.4121  False\n",
            "                    gpt-4-turbo@openai           llama-3-8b-chat@fireworks-ai   1.2489 0.7303  -0.6171  3.1149  False\n",
            "                    gpt-4-turbo@openai      mistral-7b-instruct-v0.1@anyscale    1.466    1.0  -3.7201  6.6521  False\n",
            "                    gpt-4-turbo@openai   mistral-7b-instruct-v0.3@together-ai   1.1133 0.8721  -0.7202  2.9469  False\n",
            "                    gpt-4-turbo@openai              mistral-large@aws-bedrock   0.7833 0.9983  -1.0502  2.6169  False\n",
            "                    gpt-4-turbo@openai               mistral-small@mistral-ai   0.0333    1.0  -1.8002  1.8669  False\n",
            "                    gpt-4-turbo@openai   mixtral-8x22b-instruct-v0.1@anyscale    1.116    1.0  -4.0701  6.3021  False\n",
            "                    gpt-4-turbo@openai  mixtral-8x22b-instruct-v0.1@deepinfra   -0.084    1.0    -1.95   1.782  False\n",
            "                    gpt-4-turbo@openai    mixtral-8x7b-instruct-v0.1@anyscale    0.506    1.0  -4.6801  5.6921  False\n",
            "                    gpt-4-turbo@openai mixtral-8x7b-instruct-v0.1@aws-bedrock   0.3696    1.0  -1.4964  2.2356  False\n",
            "                    gpt-4-turbo@openai                          original_text  -0.1727    1.0  -2.0062  1.6609  False\n",
            "                    gpt-4-turbo@openai          qwen-2-72b-instruct@deepinfra   2.0746 0.0116   0.2086  3.9406   True\n",
            "                          gpt-4@openai                          gpt-4o@openai  -0.3047    1.0  -2.1382  1.5289  False\n",
            "                          gpt-4@openai              llama-3-70b-chat@anyscale   3.6513  0.632  -1.5347  8.8374  False\n",
            "                          gpt-4@openai          llama-3-70b-chat@fireworks-ai   1.7449 0.1045  -0.1211  3.6109  False\n",
            "                          gpt-4@openai               llama-3-8b-chat@anyscale   1.4713    1.0  -3.7147  6.6574  False\n",
            "                          gpt-4@openai           llama-3-8b-chat@fireworks-ai   1.4942 0.3535  -0.3718  3.3602  False\n",
            "                          gpt-4@openai      mistral-7b-instruct-v0.1@anyscale   1.7113    1.0  -3.4747  6.8974  False\n",
            "                          gpt-4@openai   mistral-7b-instruct-v0.3@together-ai   1.3587 0.5222  -0.4749  3.1922  False\n",
            "                          gpt-4@openai              mistral-large@aws-bedrock   1.0287 0.9398  -0.8049  2.8622  False\n",
            "                          gpt-4@openai               mistral-small@mistral-ai   0.2787    1.0  -1.5549  2.1122  False\n",
            "                          gpt-4@openai   mixtral-8x22b-instruct-v0.1@anyscale   1.3613    1.0  -3.8247  6.5474  False\n",
            "                          gpt-4@openai  mixtral-8x22b-instruct-v0.1@deepinfra   0.1613    1.0  -1.7047  2.0273  False\n",
            "                          gpt-4@openai    mixtral-8x7b-instruct-v0.1@anyscale   0.7513    1.0  -4.4347  5.9374  False\n",
            "                          gpt-4@openai mixtral-8x7b-instruct-v0.1@aws-bedrock   0.6149    1.0  -1.2511  2.4809  False\n",
            "                          gpt-4@openai                          original_text   0.0727    1.0  -1.7609  1.9062  False\n",
            "                          gpt-4@openai          qwen-2-72b-instruct@deepinfra   2.3199 0.0016   0.4539  4.1859   True\n",
            "                         gpt-4o@openai              llama-3-70b-chat@anyscale    3.956 0.4582  -1.2301  9.1421  False\n",
            "                         gpt-4o@openai          llama-3-70b-chat@fireworks-ai   2.0496  0.014   0.1836  3.9156   True\n",
            "                         gpt-4o@openai               llama-3-8b-chat@anyscale    1.776    1.0  -3.4101  6.9621  False\n",
            "                         gpt-4o@openai           llama-3-8b-chat@fireworks-ai   1.7989 0.0761  -0.0671  3.6649  False\n",
            "                         gpt-4o@openai      mistral-7b-instruct-v0.1@anyscale    2.016 0.9996  -3.1701  7.2021  False\n",
            "                         gpt-4o@openai   mistral-7b-instruct-v0.3@together-ai   1.6633 0.1396  -0.1702  3.4969  False\n",
            "                         gpt-4o@openai              mistral-large@aws-bedrock   1.3333 0.5634  -0.5002  3.1669  False\n",
            "                         gpt-4o@openai               mistral-small@mistral-ai   0.5833    1.0  -1.2502  2.4169  False\n",
            "                         gpt-4o@openai   mixtral-8x22b-instruct-v0.1@anyscale    1.666    1.0  -3.5201  6.8521  False\n",
            "                         gpt-4o@openai  mixtral-8x22b-instruct-v0.1@deepinfra    0.466    1.0     -1.4   2.332  False\n",
            "                         gpt-4o@openai    mixtral-8x7b-instruct-v0.1@anyscale    1.056    1.0  -4.1301  6.2421  False\n",
            "                         gpt-4o@openai mixtral-8x7b-instruct-v0.1@aws-bedrock   0.9196 0.9868  -0.9464  2.7856  False\n",
            "                         gpt-4o@openai                          original_text   0.3773    1.0  -1.4562  2.2109  False\n",
            "                         gpt-4o@openai          qwen-2-72b-instruct@deepinfra   2.6246 0.0001   0.7586  4.4906   True\n",
            "             llama-3-70b-chat@anyscale          llama-3-70b-chat@fireworks-ai  -1.9064 0.9999   -7.104  3.2912  False\n",
            "             llama-3-70b-chat@anyscale               llama-3-8b-chat@anyscale    -2.18    1.0  -9.2813  4.9213  False\n",
            "             llama-3-70b-chat@anyscale           llama-3-8b-chat@fireworks-ai  -2.1571 0.9989  -7.3548  3.0405  False\n",
            "             llama-3-70b-chat@anyscale      mistral-7b-instruct-v0.1@anyscale    -1.94    1.0  -9.0413  5.1613  False\n",
            "             llama-3-70b-chat@anyscale   mistral-7b-instruct-v0.3@together-ai  -2.2927 0.9971  -7.4787  2.8934  False\n",
            "             llama-3-70b-chat@anyscale              mistral-large@aws-bedrock  -2.6227 0.9816  -7.8087  2.5634  False\n",
            "             llama-3-70b-chat@anyscale               mistral-small@mistral-ai  -3.3727  0.779  -8.5587  1.8134  False\n",
            "             llama-3-70b-chat@anyscale   mixtral-8x22b-instruct-v0.1@anyscale    -2.29    1.0  -9.3913  4.8113  False\n",
            "             llama-3-70b-chat@anyscale  mixtral-8x22b-instruct-v0.1@deepinfra    -3.49 0.7244  -8.6876  1.7076  False\n",
            "             llama-3-70b-chat@anyscale    mixtral-8x7b-instruct-v0.1@anyscale     -2.9 0.9992 -10.0013  4.2013  False\n",
            "             llama-3-70b-chat@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -3.0364 0.9101   -8.234  2.1612  False\n",
            "             llama-3-70b-chat@anyscale                          original_text  -3.5787 0.6726  -8.7647  1.6074  False\n",
            "             llama-3-70b-chat@anyscale          qwen-2-72b-instruct@deepinfra  -1.3314    1.0   -6.529  3.8662  False\n",
            "         llama-3-70b-chat@fireworks-ai               llama-3-8b-chat@anyscale  -0.2736    1.0  -5.4712   4.924  False\n",
            "         llama-3-70b-chat@fireworks-ai           llama-3-8b-chat@fireworks-ai  -0.2507    1.0  -2.1486  1.6472  False\n",
            "         llama-3-70b-chat@fireworks-ai      mistral-7b-instruct-v0.1@anyscale  -0.0336    1.0  -5.2312   5.164  False\n",
            "         llama-3-70b-chat@fireworks-ai   mistral-7b-instruct-v0.3@together-ai  -0.3862    1.0  -2.2522  1.4798  False\n",
            "         llama-3-70b-chat@fireworks-ai              mistral-large@aws-bedrock  -0.7162 0.9997  -2.5822  1.1498  False\n",
            "         llama-3-70b-chat@fireworks-ai               mistral-small@mistral-ai  -1.4662 0.3935  -3.3322  0.3998  False\n",
            "         llama-3-70b-chat@fireworks-ai   mixtral-8x22b-instruct-v0.1@anyscale  -0.3836    1.0  -5.5812   4.814  False\n",
            "         llama-3-70b-chat@fireworks-ai  mixtral-8x22b-instruct-v0.1@deepinfra  -1.5836 0.2719  -3.4815  0.3143  False\n",
            "         llama-3-70b-chat@fireworks-ai    mixtral-8x7b-instruct-v0.1@anyscale  -0.9936    1.0  -6.1912   4.204  False\n",
            "         llama-3-70b-chat@fireworks-ai mixtral-8x7b-instruct-v0.1@aws-bedrock    -1.13 0.8927  -3.0279  0.7679  False\n",
            "         llama-3-70b-chat@fireworks-ai                          original_text  -1.6722 0.1557  -3.5382  0.1938  False\n",
            "         llama-3-70b-chat@fireworks-ai          qwen-2-72b-instruct@deepinfra    0.575    1.0  -1.3229  2.4729  False\n",
            "              llama-3-8b-chat@anyscale           llama-3-8b-chat@fireworks-ai   0.0229    1.0  -5.1748  5.2205  False\n",
            "              llama-3-8b-chat@anyscale      mistral-7b-instruct-v0.1@anyscale     0.24    1.0  -6.8613  7.3413  False\n",
            "              llama-3-8b-chat@anyscale   mistral-7b-instruct-v0.3@together-ai  -0.1127    1.0  -5.2987  5.0734  False\n",
            "              llama-3-8b-chat@anyscale              mistral-large@aws-bedrock  -0.4427    1.0  -5.6287  4.7434  False\n",
            "              llama-3-8b-chat@anyscale               mistral-small@mistral-ai  -1.1927    1.0  -6.3787  3.9934  False\n",
            "              llama-3-8b-chat@anyscale   mixtral-8x22b-instruct-v0.1@anyscale    -0.11    1.0  -7.2113  6.9913  False\n",
            "              llama-3-8b-chat@anyscale  mixtral-8x22b-instruct-v0.1@deepinfra    -1.31    1.0  -6.5076  3.8876  False\n",
            "              llama-3-8b-chat@anyscale    mixtral-8x7b-instruct-v0.1@anyscale    -0.72    1.0  -7.8213  6.3813  False\n",
            "              llama-3-8b-chat@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.8564    1.0   -6.054  4.3412  False\n",
            "              llama-3-8b-chat@anyscale                          original_text  -1.3987    1.0  -6.5847  3.7874  False\n",
            "              llama-3-8b-chat@anyscale          qwen-2-72b-instruct@deepinfra   0.8486    1.0   -4.349  6.0462  False\n",
            "          llama-3-8b-chat@fireworks-ai      mistral-7b-instruct-v0.1@anyscale   0.2171    1.0  -4.9805  5.4148  False\n",
            "          llama-3-8b-chat@fireworks-ai   mistral-7b-instruct-v0.3@together-ai  -0.1355    1.0  -2.0015  1.7305  False\n",
            "          llama-3-8b-chat@fireworks-ai              mistral-large@aws-bedrock  -0.4655    1.0  -2.3315  1.4005  False\n",
            "          llama-3-8b-chat@fireworks-ai               mistral-small@mistral-ai  -1.2155 0.7764  -3.0815  0.6505  False\n",
            "          llama-3-8b-chat@fireworks-ai   mixtral-8x22b-instruct-v0.1@anyscale  -0.1329    1.0  -5.3305  5.0648  False\n",
            "          llama-3-8b-chat@fireworks-ai  mixtral-8x22b-instruct-v0.1@deepinfra  -1.3329 0.6373  -3.2308   0.565  False\n",
            "          llama-3-8b-chat@fireworks-ai    mixtral-8x7b-instruct-v0.1@anyscale  -0.7429    1.0  -5.9405  4.4548  False\n",
            "          llama-3-8b-chat@fireworks-ai mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.8793 0.9943  -2.7772  1.0186  False\n",
            "          llama-3-8b-chat@fireworks-ai                          original_text  -1.4215 0.4611  -3.2875  0.4445  False\n",
            "          llama-3-8b-chat@fireworks-ai          qwen-2-72b-instruct@deepinfra   0.8257 0.9977  -1.0722  2.7236  False\n",
            "     mistral-7b-instruct-v0.1@anyscale   mistral-7b-instruct-v0.3@together-ai  -0.3527    1.0  -5.5387  4.8334  False\n",
            "     mistral-7b-instruct-v0.1@anyscale              mistral-large@aws-bedrock  -0.6827    1.0  -5.8687  4.5034  False\n",
            "     mistral-7b-instruct-v0.1@anyscale               mistral-small@mistral-ai  -1.4327    1.0  -6.6187  3.7534  False\n",
            "     mistral-7b-instruct-v0.1@anyscale   mixtral-8x22b-instruct-v0.1@anyscale    -0.35    1.0  -7.4513  6.7513  False\n",
            "     mistral-7b-instruct-v0.1@anyscale  mixtral-8x22b-instruct-v0.1@deepinfra    -1.55    1.0  -6.7476  3.6476  False\n",
            "     mistral-7b-instruct-v0.1@anyscale    mixtral-8x7b-instruct-v0.1@anyscale    -0.96    1.0  -8.0613  6.1413  False\n",
            "     mistral-7b-instruct-v0.1@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.0964    1.0   -6.294  4.1012  False\n",
            "     mistral-7b-instruct-v0.1@anyscale                          original_text  -1.6387    1.0  -6.8247  3.5474  False\n",
            "     mistral-7b-instruct-v0.1@anyscale          qwen-2-72b-instruct@deepinfra   0.6086    1.0   -4.589  5.8062  False\n",
            "  mistral-7b-instruct-v0.3@together-ai              mistral-large@aws-bedrock    -0.33    1.0  -2.1635  1.5035  False\n",
            "  mistral-7b-instruct-v0.3@together-ai               mistral-small@mistral-ai    -1.08 0.9028  -2.9135  0.7535  False\n",
            "  mistral-7b-instruct-v0.3@together-ai   mixtral-8x22b-instruct-v0.1@anyscale   0.0027    1.0  -5.1834  5.1887  False\n",
            "  mistral-7b-instruct-v0.3@together-ai  mixtral-8x22b-instruct-v0.1@deepinfra  -1.1973 0.7999  -3.0633  0.6687  False\n",
            "  mistral-7b-instruct-v0.3@together-ai    mixtral-8x7b-instruct-v0.1@anyscale  -0.6073    1.0  -5.7934  4.5787  False\n",
            "  mistral-7b-instruct-v0.3@together-ai mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.7438 0.9994  -2.6098  1.1222  False\n",
            "  mistral-7b-instruct-v0.3@together-ai                          original_text   -1.286 0.6399  -3.1195  0.5475  False\n",
            "  mistral-7b-instruct-v0.3@together-ai          qwen-2-72b-instruct@deepinfra   0.9612 0.9769  -0.9048  2.8272  False\n",
            "             mistral-large@aws-bedrock               mistral-small@mistral-ai    -0.75 0.9991  -2.5835  1.0835  False\n",
            "             mistral-large@aws-bedrock   mixtral-8x22b-instruct-v0.1@anyscale   0.3327    1.0  -4.8534  5.5187  False\n",
            "             mistral-large@aws-bedrock  mixtral-8x22b-instruct-v0.1@deepinfra  -0.8673  0.994  -2.7333  0.9987  False\n",
            "             mistral-large@aws-bedrock    mixtral-8x7b-instruct-v0.1@anyscale  -0.2773    1.0  -5.4634  4.9087  False\n",
            "             mistral-large@aws-bedrock mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.4138    1.0  -2.2798  1.4522  False\n",
            "             mistral-large@aws-bedrock                          original_text   -0.956 0.9733  -2.7895  0.8775  False\n",
            "             mistral-large@aws-bedrock          qwen-2-72b-instruct@deepinfra   1.2912 0.6671  -0.5748  3.1572  False\n",
            "              mistral-small@mistral-ai   mixtral-8x22b-instruct-v0.1@anyscale   1.0827    1.0  -4.1034  6.2687  False\n",
            "              mistral-small@mistral-ai  mixtral-8x22b-instruct-v0.1@deepinfra  -0.1173    1.0  -1.9833  1.7487  False\n",
            "              mistral-small@mistral-ai    mixtral-8x7b-instruct-v0.1@anyscale   0.4727    1.0  -4.7134  5.6587  False\n",
            "              mistral-small@mistral-ai mixtral-8x7b-instruct-v0.1@aws-bedrock   0.3362    1.0  -1.5298  2.2022  False\n",
            "              mistral-small@mistral-ai                          original_text   -0.206    1.0  -2.0395  1.6275  False\n",
            "              mistral-small@mistral-ai          qwen-2-72b-instruct@deepinfra   2.0412 0.0149   0.1752  3.9072   True\n",
            "  mixtral-8x22b-instruct-v0.1@anyscale  mixtral-8x22b-instruct-v0.1@deepinfra     -1.2    1.0  -6.3976  3.9976  False\n",
            "  mixtral-8x22b-instruct-v0.1@anyscale    mixtral-8x7b-instruct-v0.1@anyscale    -0.61    1.0  -7.7113  6.4913  False\n",
            "  mixtral-8x22b-instruct-v0.1@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.7464    1.0   -5.944  4.4512  False\n",
            "  mixtral-8x22b-instruct-v0.1@anyscale                          original_text  -1.2887    1.0  -6.4747  3.8974  False\n",
            "  mixtral-8x22b-instruct-v0.1@anyscale          qwen-2-72b-instruct@deepinfra   0.9586    1.0   -4.239  6.1562  False\n",
            " mixtral-8x22b-instruct-v0.1@deepinfra    mixtral-8x7b-instruct-v0.1@anyscale     0.59    1.0  -4.6076  5.7876  False\n",
            " mixtral-8x22b-instruct-v0.1@deepinfra mixtral-8x7b-instruct-v0.1@aws-bedrock   0.4536    1.0  -1.4443  2.3515  False\n",
            " mixtral-8x22b-instruct-v0.1@deepinfra                          original_text  -0.0887    1.0  -1.9547  1.7773  False\n",
            " mixtral-8x22b-instruct-v0.1@deepinfra          qwen-2-72b-instruct@deepinfra   2.1586 0.0081   0.2607  4.0565   True\n",
            "   mixtral-8x7b-instruct-v0.1@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.1364    1.0   -5.334  5.0612  False\n",
            "   mixtral-8x7b-instruct-v0.1@anyscale                          original_text  -0.6787    1.0  -5.8647  4.5074  False\n",
            "   mixtral-8x7b-instruct-v0.1@anyscale          qwen-2-72b-instruct@deepinfra   1.5686    1.0   -3.629  6.7662  False\n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock                          original_text  -0.5422    1.0  -2.4082  1.3238  False\n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock          qwen-2-72b-instruct@deepinfra    1.705 0.1523  -0.1929  3.6029  False\n",
            "                         original_text          qwen-2-72b-instruct@deepinfra   2.2472  0.003   0.3812  4.1132   True\n",
            "---------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_table = filtered_df.groupby('model')['Gunning Fog'].describe()\n",
        "anova_df = pd.DataFrame(anova_table)\n",
        "\n",
        "output_file_path = 'summary_and_anova_output_Gunning Fog.xlsx'\n",
        "with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:\n",
        "    summary_table.to_excel(writer, sheet_name='Summary Statistics')\n",
        "    anova_df.to_excel(writer, sheet_name='ANOVA Table')\n",
        "\n",
        "print(f\"Data has been saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev3TTFN_0q2B",
        "outputId": "2bc88781-25a2-4da4-f29f-361f75f557b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to summary_and_anova_output_Gunning Fog.xlsx\n"
          ]
        }
      ]
    }
  ]
}