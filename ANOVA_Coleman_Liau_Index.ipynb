{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xVcpPljBhb7f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.stats.multicomp as mc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FarNokLNhs_f",
        "outputId": "f344a2d1-4119-4bde-8c9d-99dd26912e0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = pd.read_excel('overalldata.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "exclude_models = [\n",
        "    \"codellama-70b-instruct@anyscale\",\n",
        "    \"codellama-7b-instruct@together-ai\",\n",
        "    \"command-r-plus@aws-bedrock\",\n",
        "    \"deepseek-coder-33b-instruct@together-ai\",\n",
        "    \"phi-3-medium-4k-instruct@deepinfra\"\n",
        "]\n",
        "\n",
        "filtered_df = df_new[~df_new['model'].isin(exclude_models)]\n",
        "\n",
        "control_group = filtered_df[filtered_df['model'] == 'original_text']\n",
        "other_groups = filtered_df[filtered_df['model'] != 'original_text']\n",
        "combined_df = pd.concat([control_group, other_groups])\n",
        "\n",
        "model = ols('Q(\"Coleman-Liau Index\") ~ C(model)', data=combined_df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(anova_table)\n",
        "\n",
        "comp = mc.pairwise_tukeyhsd(combined_df['Coleman-Liau Index'], combined_df['model'])\n",
        "tukey_results = pd.DataFrame(data=comp.summary().data[1:], columns=comp.summary().data[0])\n",
        "\n",
        "control_comparisons = tukey_results[tukey_results['group1'] == 'original_text']\n",
        "\n",
        "control_comparisons_sorted = control_comparisons.sort_values(by='meandiff', key=abs)\n",
        "print(control_comparisons_sorted.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4f-tEkThvyn",
        "outputId": "df30f31d-30a5-4a88-9b20-6ce2ad4dce8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              sum_sq     df         F        PR(>F)\n",
            "C(model)  200.802861   28.0  4.561794  1.323822e-11\n",
            "Residual  430.751429  274.0       NaN           NaN\n",
            "            group1                         group2  meandiff  p-adj   lower  \\\n",
            "405  original_text  qwen-2-72b-instruct@deepinfra    3.4463    0.0  1.6887   \n",
            "\n",
            "     upper  reject  \n",
            "405  5.204    True  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.stats.multicomp as mc\n",
        "\n",
        "df_new = pd.read_excel('overalldata.xlsx', sheet_name='Sheet1')\n",
        "exclude_models = [\n",
        "    \"codellama-70b-instruct@anyscale\",\n",
        "    \"codellama-7b-instruct@together-ai\",\n",
        "    \"command-r-plus@aws-bedrock\",\n",
        "    \"deepseek-coder-33b-instruct@together-ai\",\n",
        "    \"phi-3-medium-4k-instruct@deepinfra\",\n",
        "    \"nemotron-4-340b-instruct@deepinfra\",\n",
        "    \"codellama-13b-instruct@together-ai\"\n",
        "]\n",
        "\n",
        "filtered_df = df_new[~df_new['model'].isin(exclude_models)]\n",
        "\n",
        "summary_table = filtered_df.groupby('model')['SMOG Index'].describe()\n",
        "\n",
        "\n",
        "print(summary_table)\n",
        "model = ols('Q(\"Coleman-Liau Index\") ~ C(model)', data=filtered_df).fit()\n",
        "\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(anova_table)\n",
        "anova_table.to_excel('anova_table_Coleman-Liau Index.xlsx', index=True)\n",
        "\n",
        "tukey = mc.pairwise_tukeyhsd(filtered_df['Coleman-Liau Index'], filtered_df['model'])\n",
        "print(tukey.summary())\n",
        "\n",
        "tukey_summary_df = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])\n",
        "tukey_summary_df.to_excel('tukey_summary_Coleman-Liau Index.xlsx', index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaI6k3C1iLfv",
        "outputId": "a14da1b5-555d-49c7-e4c9-cbbfa39c5ab2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        count       mean       std   min  \\\n",
            "model                                                                      \n",
            "claude-3-haiku@anthropic                 15.0   9.220000  1.081137   7.4   \n",
            "claude-3-opus@anthropic                  15.0   9.126667  1.181081   6.9   \n",
            "claude-3-sonnet@anthropic                 1.0  11.400000       NaN  11.4   \n",
            "claude-3.5-sonnet@anthropic              15.0   8.846667  1.128758   6.4   \n",
            "gemini-1.5-flash@vertex-ai               15.0   8.733333  1.119098   6.8   \n",
            "gemini-1.5-pro@vertex-ai                 15.0   8.433333  0.718795   7.4   \n",
            "gemma-2-9b-it@fireworks-ai               15.0   8.533333  1.211650   6.2   \n",
            "gemma-2b-it@together-ai                  14.0   9.207143  1.219372   7.1   \n",
            "gemma-7b-it@anyscale                      1.0  10.100000       NaN  10.1   \n",
            "gpt-3.5-turbo@openai                     15.0   9.300000  1.281740   7.7   \n",
            "gpt-4-turbo@openai                       15.0   8.293333  0.786917   6.9   \n",
            "gpt-4@openai                             15.0   8.813333  0.896714   7.6   \n",
            "gpt-4o@openai                            15.0   8.586667  0.709997   7.5   \n",
            "llama-3-70b-chat@anyscale                 1.0  11.300000       NaN  11.3   \n",
            "llama-3-70b-chat@fireworks-ai            14.0   9.557143  1.251900   7.6   \n",
            "llama-3-8b-chat@anyscale                  1.0   9.100000       NaN   9.1   \n",
            "llama-3-8b-chat@fireworks-ai             14.0   9.507143  1.464169   7.0   \n",
            "mistral-7b-instruct-v0.1@anyscale         1.0  10.200000       NaN  10.2   \n",
            "mistral-7b-instruct-v0.3@together-ai     15.0   9.633333  1.217531   7.0   \n",
            "mistral-large@aws-bedrock                15.0   9.346667  0.879827   8.0   \n",
            "mistral-small@mistral-ai                 15.0   8.720000  1.095576   6.8   \n",
            "mixtral-8x22b-instruct-v0.1@anyscale      1.0   8.800000       NaN   8.8   \n",
            "mixtral-8x22b-instruct-v0.1@deepinfra    14.0   8.942857  0.676773   7.9   \n",
            "mixtral-8x7b-instruct-v0.1@anyscale       1.0   9.700000       NaN   9.7   \n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock   14.0   7.985714  2.523995   0.0   \n",
            "original_text                            15.0   6.340000  2.152009   3.1   \n",
            "qwen-2-72b-instruct@deepinfra            14.0   9.935714  0.826119   8.5   \n",
            "\n",
            "                                           25%    50%     75%   max  \n",
            "model                                                                \n",
            "claude-3-haiku@anthropic                 8.550   9.10  10.200  10.8  \n",
            "claude-3-opus@anthropic                  8.500   9.00   9.800  11.9  \n",
            "claude-3-sonnet@anthropic               11.400  11.40  11.400  11.4  \n",
            "claude-3.5-sonnet@anthropic              8.100   8.80   9.600  10.6  \n",
            "gemini-1.5-flash@vertex-ai               8.050   8.50   9.600  10.9  \n",
            "gemini-1.5-pro@vertex-ai                 7.950   8.40   8.950   9.6  \n",
            "gemma-2-9b-it@fireworks-ai               7.800   8.60   8.900  10.9  \n",
            "gemma-2b-it@together-ai                  8.525   9.75  10.175  10.4  \n",
            "gemma-7b-it@anyscale                    10.100  10.10  10.100  10.1  \n",
            "gpt-3.5-turbo@openai                     8.300   9.20   9.900  12.2  \n",
            "gpt-4-turbo@openai                       7.750   8.20   8.650   9.8  \n",
            "gpt-4@openai                             8.250   8.80   9.100  11.0  \n",
            "gpt-4o@openai                            8.050   8.70   9.000   9.9  \n",
            "llama-3-70b-chat@anyscale               11.300  11.30  11.300  11.3  \n",
            "llama-3-70b-chat@fireworks-ai            8.725   9.40  10.075  11.8  \n",
            "llama-3-8b-chat@anyscale                 9.100   9.10   9.100   9.1  \n",
            "llama-3-8b-chat@fireworks-ai             8.650   9.45  10.100  12.4  \n",
            "mistral-7b-instruct-v0.1@anyscale       10.200  10.20  10.200  10.2  \n",
            "mistral-7b-instruct-v0.3@together-ai     8.900   9.80  10.400  11.4  \n",
            "mistral-large@aws-bedrock                8.600   9.70   9.800  10.9  \n",
            "mistral-small@mistral-ai                 7.800   8.80   9.450  10.8  \n",
            "mixtral-8x22b-instruct-v0.1@anyscale     8.800   8.80   8.800   8.8  \n",
            "mixtral-8x22b-instruct-v0.1@deepinfra    8.475   8.95   9.500  10.0  \n",
            "mixtral-8x7b-instruct-v0.1@anyscale      9.700   9.70   9.700   9.7  \n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock   7.800   8.45   9.175  11.0  \n",
            "original_text                            5.500   7.00   7.350  11.2  \n",
            "qwen-2-72b-instruct@deepinfra            9.550   9.85  10.300  12.1  \n",
            "              sum_sq     df         F        PR(>F)\n",
            "C(model)  196.572645   26.0  4.809208  7.501946e-12\n",
            "Residual  430.751429  274.0       NaN           NaN\n",
            "                                 Multiple Comparison of Means - Tukey HSD, FWER=0.05                                 \n",
            "=====================================================================================================================\n",
            "                group1                                 group2                 meandiff p-adj   lower    upper  reject\n",
            "---------------------------------------------------------------------------------------------------------------------\n",
            "              claude-3-haiku@anthropic                claude-3-opus@anthropic   0.2373    1.0  -1.4731  1.9477  False\n",
            "              claude-3-haiku@anthropic              claude-3-sonnet@anthropic    2.346 0.9893  -2.4917  7.1837  False\n",
            "              claude-3-haiku@anthropic            claude-3.5-sonnet@anthropic   0.1627    1.0  -1.5477  1.8731  False\n",
            "              claude-3-haiku@anthropic             gemini-1.5-flash@vertex-ai    0.276    1.0  -1.4344  1.9864  False\n",
            "              claude-3-haiku@anthropic               gemini-1.5-pro@vertex-ai    0.276    1.0  -1.4344  1.9864  False\n",
            "              claude-3-haiku@anthropic             gemma-2-9b-it@fireworks-ai    0.812 0.9919  -0.8984  2.5224  False\n",
            "              claude-3-haiku@anthropic                gemma-2b-it@together-ai   0.6017    1.0   -1.139  2.3424  False\n",
            "              claude-3-haiku@anthropic                   gemma-7b-it@anyscale    2.516 0.9741  -2.3217  7.3537  False\n",
            "              claude-3-haiku@anthropic                   gpt-3.5-turbo@openai   0.1487    1.0  -1.5617  1.8591  False\n",
            "              claude-3-haiku@anthropic                     gpt-4-turbo@openai   0.0087    1.0  -1.7017  1.7191  False\n",
            "              claude-3-haiku@anthropic                           gpt-4@openai  -0.3233    1.0  -2.0337  1.3871  False\n",
            "              claude-3-haiku@anthropic                          gpt-4o@openai  -0.0287    1.0  -1.7391  1.6817  False\n",
            "              claude-3-haiku@anthropic              llama-3-70b-chat@anyscale    2.926 0.8764  -1.9117  7.7637  False\n",
            "              claude-3-haiku@anthropic          llama-3-70b-chat@fireworks-ai   1.2796 0.5398  -0.4611  3.0202  False\n",
            "              claude-3-haiku@anthropic               llama-3-8b-chat@anyscale    0.486    1.0  -4.3517  5.3237  False\n",
            "              claude-3-haiku@anthropic           llama-3-8b-chat@fireworks-ai   0.6396 0.9999  -1.1011  2.3802  False\n",
            "              claude-3-haiku@anthropic      mistral-7b-instruct-v0.1@anyscale   -0.564    1.0  -5.4017  4.2737  False\n",
            "              claude-3-haiku@anthropic   mistral-7b-instruct-v0.3@together-ai      0.7 0.9991  -1.0104  2.4104  False\n",
            "              claude-3-haiku@anthropic              mistral-large@aws-bedrock   0.4453    1.0  -1.2651  2.1557  False\n",
            "              claude-3-haiku@anthropic               mistral-small@mistral-ai  -0.4213    1.0  -2.1317  1.2891  False\n",
            "              claude-3-haiku@anthropic   mixtral-8x22b-instruct-v0.1@anyscale    0.136    1.0  -4.7017  4.9737  False\n",
            "              claude-3-haiku@anthropic  mixtral-8x22b-instruct-v0.1@deepinfra    0.031    1.0  -1.7097  1.7717  False\n",
            "              claude-3-haiku@anthropic    mixtral-8x7b-instruct-v0.1@anyscale   -0.274    1.0  -5.1117  4.5637  False\n",
            "              claude-3-haiku@anthropic mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.8983 0.9764   -2.639  0.8424  False\n",
            "              claude-3-haiku@anthropic                          original_text  -2.5253    0.0  -4.2357 -0.8149   True\n",
            "              claude-3-haiku@anthropic          qwen-2-72b-instruct@deepinfra    0.921 0.9683  -0.8197  2.6617  False\n",
            "               claude-3-opus@anthropic              claude-3-sonnet@anthropic   2.1087 0.9976  -2.7291  6.9464  False\n",
            "               claude-3-opus@anthropic            claude-3.5-sonnet@anthropic  -0.0747    1.0  -1.7851  1.6357  False\n",
            "               claude-3-opus@anthropic             gemini-1.5-flash@vertex-ai   0.0387    1.0  -1.6717  1.7491  False\n",
            "               claude-3-opus@anthropic               gemini-1.5-pro@vertex-ai   0.0387    1.0  -1.6717  1.7491  False\n",
            "               claude-3-opus@anthropic             gemma-2-9b-it@fireworks-ai   0.5747    1.0  -1.1357  2.2851  False\n",
            "               claude-3-opus@anthropic                gemma-2b-it@together-ai   0.3644    1.0  -1.3763  2.1051  False\n",
            "               claude-3-opus@anthropic                   gemma-7b-it@anyscale   2.2787 0.9928  -2.5591  7.1164  False\n",
            "               claude-3-opus@anthropic                   gpt-3.5-turbo@openai  -0.0887    1.0  -1.7991  1.6217  False\n",
            "               claude-3-opus@anthropic                     gpt-4-turbo@openai  -0.2287    1.0  -1.9391  1.4817  False\n",
            "               claude-3-opus@anthropic                           gpt-4@openai  -0.5607    1.0  -2.2711  1.1497  False\n",
            "               claude-3-opus@anthropic                          gpt-4o@openai   -0.266    1.0  -1.9764  1.4444  False\n",
            "               claude-3-opus@anthropic              llama-3-70b-chat@anyscale   2.6887 0.9455  -2.1491  7.5264  False\n",
            "               claude-3-opus@anthropic          llama-3-70b-chat@fireworks-ai   1.0422  0.887  -0.6984  2.7829  False\n",
            "               claude-3-opus@anthropic               llama-3-8b-chat@anyscale   0.2487    1.0  -4.5891  5.0864  False\n",
            "               claude-3-opus@anthropic           llama-3-8b-chat@fireworks-ai   0.4022    1.0  -1.3384  2.1429  False\n",
            "               claude-3-opus@anthropic      mistral-7b-instruct-v0.1@anyscale  -0.8013    1.0  -5.6391  4.0364  False\n",
            "               claude-3-opus@anthropic   mistral-7b-instruct-v0.3@together-ai   0.4627    1.0  -1.2477  2.1731  False\n",
            "               claude-3-opus@anthropic              mistral-large@aws-bedrock    0.208    1.0  -1.5024  1.9184  False\n",
            "               claude-3-opus@anthropic               mistral-small@mistral-ai  -0.6587 0.9997  -2.3691  1.0517  False\n",
            "               claude-3-opus@anthropic   mixtral-8x22b-instruct-v0.1@anyscale  -0.1013    1.0  -4.9391  4.7364  False\n",
            "               claude-3-opus@anthropic  mixtral-8x22b-instruct-v0.1@deepinfra  -0.2063    1.0   -1.947  1.5343  False\n",
            "               claude-3-opus@anthropic    mixtral-8x7b-instruct-v0.1@anyscale  -0.5113    1.0  -5.3491  4.3264  False\n",
            "               claude-3-opus@anthropic mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.1356 0.7739  -2.8763  0.6051  False\n",
            "               claude-3-opus@anthropic                          original_text  -2.7627    0.0  -4.4731 -1.0523   True\n",
            "               claude-3-opus@anthropic          qwen-2-72b-instruct@deepinfra   0.6837 0.9996   -1.057  2.4243  False\n",
            "             claude-3-sonnet@anthropic            claude-3.5-sonnet@anthropic  -2.1833 0.9961  -7.0211  2.6544  False\n",
            "             claude-3-sonnet@anthropic             gemini-1.5-flash@vertex-ai    -2.07 0.9982  -6.9077  2.7677  False\n",
            "             claude-3-sonnet@anthropic               gemini-1.5-pro@vertex-ai    -2.07 0.9982  -6.9077  2.7677  False\n",
            "             claude-3-sonnet@anthropic             gemma-2-9b-it@fireworks-ai   -1.534    1.0  -6.3717  3.3037  False\n",
            "             claude-3-sonnet@anthropic                gemma-2b-it@together-ai  -1.7443 0.9999  -6.5928  3.1042  False\n",
            "             claude-3-sonnet@anthropic                   gemma-7b-it@anyscale     0.17    1.0  -6.4544  6.7944  False\n",
            "             claude-3-sonnet@anthropic                   gpt-3.5-turbo@openai  -2.1973 0.9957  -7.0351  2.6404  False\n",
            "             claude-3-sonnet@anthropic                     gpt-4-turbo@openai  -2.3373 0.9898  -7.1751  2.5004  False\n",
            "             claude-3-sonnet@anthropic                           gpt-4@openai  -2.6693 0.9495  -7.5071  2.1684  False\n",
            "             claude-3-sonnet@anthropic                          gpt-4o@openai  -2.3747 0.9874  -7.2124  2.4631  False\n",
            "             claude-3-sonnet@anthropic              llama-3-70b-chat@anyscale     0.58    1.0  -6.0444  7.2044  False\n",
            "             claude-3-sonnet@anthropic          llama-3-70b-chat@fireworks-ai  -1.0664    1.0   -5.915  3.7821  False\n",
            "             claude-3-sonnet@anthropic               llama-3-8b-chat@anyscale    -1.86    1.0  -8.4844  4.7644  False\n",
            "             claude-3-sonnet@anthropic           llama-3-8b-chat@fireworks-ai  -1.7064 0.9999   -6.555  3.1421  False\n",
            "             claude-3-sonnet@anthropic      mistral-7b-instruct-v0.1@anyscale    -2.91 0.9974  -9.5344  3.7144  False\n",
            "             claude-3-sonnet@anthropic   mistral-7b-instruct-v0.3@together-ai   -1.646    1.0  -6.4837  3.1917  False\n",
            "             claude-3-sonnet@anthropic              mistral-large@aws-bedrock  -1.9007 0.9996  -6.7384  2.9371  False\n",
            "             claude-3-sonnet@anthropic               mistral-small@mistral-ai  -2.7673 0.9267  -7.6051  2.0704  False\n",
            "             claude-3-sonnet@anthropic   mixtral-8x22b-instruct-v0.1@anyscale    -2.21    1.0  -8.8344  4.4144  False\n",
            "             claude-3-sonnet@anthropic  mixtral-8x22b-instruct-v0.1@deepinfra   -2.315 0.9913  -7.1635  2.5335  False\n",
            "             claude-3-sonnet@anthropic    mixtral-8x7b-instruct-v0.1@anyscale    -2.62 0.9995  -9.2444  4.0044  False\n",
            "             claude-3-sonnet@anthropic mixtral-8x7b-instruct-v0.1@aws-bedrock  -3.2443 0.7306  -8.0928  1.6042  False\n",
            "             claude-3-sonnet@anthropic                          original_text  -4.8713  0.046  -9.7091 -0.0336   True\n",
            "             claude-3-sonnet@anthropic          qwen-2-72b-instruct@deepinfra   -1.425    1.0  -6.2735  3.4235  False\n",
            "           claude-3.5-sonnet@anthropic             gemini-1.5-flash@vertex-ai   0.1133    1.0  -1.5971  1.8237  False\n",
            "           claude-3.5-sonnet@anthropic               gemini-1.5-pro@vertex-ai   0.1133    1.0  -1.5971  1.8237  False\n",
            "           claude-3.5-sonnet@anthropic             gemma-2-9b-it@fireworks-ai   0.6493 0.9998  -1.0611  2.3597  False\n",
            "           claude-3.5-sonnet@anthropic                gemma-2b-it@together-ai    0.439    1.0  -1.3016  2.1797  False\n",
            "           claude-3.5-sonnet@anthropic                   gemma-7b-it@anyscale   2.3533 0.9888  -2.4844  7.1911  False\n",
            "           claude-3.5-sonnet@anthropic                   gpt-3.5-turbo@openai   -0.014    1.0  -1.7244  1.6964  False\n",
            "           claude-3.5-sonnet@anthropic                     gpt-4-turbo@openai   -0.154    1.0  -1.8644  1.5564  False\n",
            "           claude-3.5-sonnet@anthropic                           gpt-4@openai   -0.486    1.0  -2.1964  1.2244  False\n",
            "           claude-3.5-sonnet@anthropic                          gpt-4o@openai  -0.1913    1.0  -1.9017  1.5191  False\n",
            "           claude-3.5-sonnet@anthropic              llama-3-70b-chat@anyscale   2.7633 0.9278  -2.0744  7.6011  False\n",
            "           claude-3.5-sonnet@anthropic          llama-3-70b-chat@fireworks-ai   1.1169 0.7999  -0.6238  2.8576  False\n",
            "           claude-3.5-sonnet@anthropic               llama-3-8b-chat@anyscale   0.3233    1.0  -4.5144  5.1611  False\n",
            "           claude-3.5-sonnet@anthropic           llama-3-8b-chat@fireworks-ai   0.4769    1.0  -1.2638  2.2176  False\n",
            "           claude-3.5-sonnet@anthropic      mistral-7b-instruct-v0.1@anyscale  -0.7267    1.0  -5.5644  4.1111  False\n",
            "           claude-3.5-sonnet@anthropic   mistral-7b-instruct-v0.3@together-ai   0.5373    1.0  -1.1731  2.2477  False\n",
            "           claude-3.5-sonnet@anthropic              mistral-large@aws-bedrock   0.2827    1.0  -1.4277  1.9931  False\n",
            "           claude-3.5-sonnet@anthropic               mistral-small@mistral-ai   -0.584    1.0  -2.2944  1.1264  False\n",
            "           claude-3.5-sonnet@anthropic   mixtral-8x22b-instruct-v0.1@anyscale  -0.0267    1.0  -4.8644  4.8111  False\n",
            "           claude-3.5-sonnet@anthropic  mixtral-8x22b-instruct-v0.1@deepinfra  -0.1317    1.0  -1.8723   1.609  False\n",
            "           claude-3.5-sonnet@anthropic    mixtral-8x7b-instruct-v0.1@anyscale  -0.4367    1.0  -5.2744  4.4011  False\n",
            "           claude-3.5-sonnet@anthropic mixtral-8x7b-instruct-v0.1@aws-bedrock   -1.061 0.8679  -2.8016  0.6797  False\n",
            "           claude-3.5-sonnet@anthropic                          original_text   -2.688    0.0  -4.3984 -0.9776   True\n",
            "           claude-3.5-sonnet@anthropic          qwen-2-72b-instruct@deepinfra   0.7583 0.9977  -0.9823   2.499  False\n",
            "            gemini-1.5-flash@vertex-ai               gemini-1.5-pro@vertex-ai      0.0    1.0  -1.7104  1.7104  False\n",
            "            gemini-1.5-flash@vertex-ai             gemma-2-9b-it@fireworks-ai    0.536    1.0  -1.1744  2.2464  False\n",
            "            gemini-1.5-flash@vertex-ai                gemma-2b-it@together-ai   0.3257    1.0   -1.415  2.0664  False\n",
            "            gemini-1.5-flash@vertex-ai                   gemma-7b-it@anyscale     2.24 0.9943  -2.5977  7.0777  False\n",
            "            gemini-1.5-flash@vertex-ai                   gpt-3.5-turbo@openai  -0.1273    1.0  -1.8377  1.5831  False\n",
            "            gemini-1.5-flash@vertex-ai                     gpt-4-turbo@openai  -0.2673    1.0  -1.9777  1.4431  False\n",
            "            gemini-1.5-flash@vertex-ai                           gpt-4@openai  -0.5993 0.9999  -2.3097  1.1111  False\n",
            "            gemini-1.5-flash@vertex-ai                          gpt-4o@openai  -0.3047    1.0  -2.0151  1.4057  False\n",
            "            gemini-1.5-flash@vertex-ai              llama-3-70b-chat@anyscale     2.65 0.9533  -2.1877  7.4877  False\n",
            "            gemini-1.5-flash@vertex-ai          llama-3-70b-chat@fireworks-ai   1.0036 0.9208  -0.7371  2.7442  False\n",
            "            gemini-1.5-flash@vertex-ai               llama-3-8b-chat@anyscale     0.21    1.0  -4.6277  5.0477  False\n",
            "            gemini-1.5-flash@vertex-ai           llama-3-8b-chat@fireworks-ai   0.3636    1.0  -1.3771  2.1042  False\n",
            "            gemini-1.5-flash@vertex-ai      mistral-7b-instruct-v0.1@anyscale    -0.84    1.0  -5.6777  3.9977  False\n",
            "            gemini-1.5-flash@vertex-ai   mistral-7b-instruct-v0.3@together-ai    0.424    1.0  -1.2864  2.1344  False\n",
            "            gemini-1.5-flash@vertex-ai              mistral-large@aws-bedrock   0.1693    1.0  -1.5411  1.8797  False\n",
            "            gemini-1.5-flash@vertex-ai               mistral-small@mistral-ai  -0.6973 0.9992  -2.4077  1.0131  False\n",
            "            gemini-1.5-flash@vertex-ai   mixtral-8x22b-instruct-v0.1@anyscale    -0.14    1.0  -4.9777  4.6977  False\n",
            "            gemini-1.5-flash@vertex-ai  mixtral-8x22b-instruct-v0.1@deepinfra   -0.245    1.0  -1.9857  1.4957  False\n",
            "            gemini-1.5-flash@vertex-ai    mixtral-8x7b-instruct-v0.1@anyscale    -0.55    1.0  -5.3877  4.2877  False\n",
            "            gemini-1.5-flash@vertex-ai mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.1743 0.7158   -2.915  0.5664  False\n",
            "            gemini-1.5-flash@vertex-ai                          original_text  -2.8013    0.0  -4.5117 -1.0909   True\n",
            "            gemini-1.5-flash@vertex-ai          qwen-2-72b-instruct@deepinfra    0.645 0.9998  -1.0957  2.3857  False\n",
            "              gemini-1.5-pro@vertex-ai             gemma-2-9b-it@fireworks-ai    0.536    1.0  -1.1744  2.2464  False\n",
            "              gemini-1.5-pro@vertex-ai                gemma-2b-it@together-ai   0.3257    1.0   -1.415  2.0664  False\n",
            "              gemini-1.5-pro@vertex-ai                   gemma-7b-it@anyscale     2.24 0.9943  -2.5977  7.0777  False\n",
            "              gemini-1.5-pro@vertex-ai                   gpt-3.5-turbo@openai  -0.1273    1.0  -1.8377  1.5831  False\n",
            "              gemini-1.5-pro@vertex-ai                     gpt-4-turbo@openai  -0.2673    1.0  -1.9777  1.4431  False\n",
            "              gemini-1.5-pro@vertex-ai                           gpt-4@openai  -0.5993 0.9999  -2.3097  1.1111  False\n",
            "              gemini-1.5-pro@vertex-ai                          gpt-4o@openai  -0.3047    1.0  -2.0151  1.4057  False\n",
            "              gemini-1.5-pro@vertex-ai              llama-3-70b-chat@anyscale     2.65 0.9533  -2.1877  7.4877  False\n",
            "              gemini-1.5-pro@vertex-ai          llama-3-70b-chat@fireworks-ai   1.0036 0.9208  -0.7371  2.7442  False\n",
            "              gemini-1.5-pro@vertex-ai               llama-3-8b-chat@anyscale     0.21    1.0  -4.6277  5.0477  False\n",
            "              gemini-1.5-pro@vertex-ai           llama-3-8b-chat@fireworks-ai   0.3636    1.0  -1.3771  2.1042  False\n",
            "              gemini-1.5-pro@vertex-ai      mistral-7b-instruct-v0.1@anyscale    -0.84    1.0  -5.6777  3.9977  False\n",
            "              gemini-1.5-pro@vertex-ai   mistral-7b-instruct-v0.3@together-ai    0.424    1.0  -1.2864  2.1344  False\n",
            "              gemini-1.5-pro@vertex-ai              mistral-large@aws-bedrock   0.1693    1.0  -1.5411  1.8797  False\n",
            "              gemini-1.5-pro@vertex-ai               mistral-small@mistral-ai  -0.6973 0.9992  -2.4077  1.0131  False\n",
            "              gemini-1.5-pro@vertex-ai   mixtral-8x22b-instruct-v0.1@anyscale    -0.14    1.0  -4.9777  4.6977  False\n",
            "              gemini-1.5-pro@vertex-ai  mixtral-8x22b-instruct-v0.1@deepinfra   -0.245    1.0  -1.9857  1.4957  False\n",
            "              gemini-1.5-pro@vertex-ai    mixtral-8x7b-instruct-v0.1@anyscale    -0.55    1.0  -5.3877  4.2877  False\n",
            "              gemini-1.5-pro@vertex-ai mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.1743 0.7158   -2.915  0.5664  False\n",
            "              gemini-1.5-pro@vertex-ai                          original_text  -2.8013    0.0  -4.5117 -1.0909   True\n",
            "              gemini-1.5-pro@vertex-ai          qwen-2-72b-instruct@deepinfra    0.645 0.9998  -1.0957  2.3857  False\n",
            "            gemma-2-9b-it@fireworks-ai                gemma-2b-it@together-ai  -0.2103    1.0   -1.951  1.5304  False\n",
            "            gemma-2-9b-it@fireworks-ai                   gemma-7b-it@anyscale    1.704 0.9999  -3.1337  6.5417  False\n",
            "            gemma-2-9b-it@fireworks-ai                   gpt-3.5-turbo@openai  -0.6633 0.9996  -2.3737  1.0471  False\n",
            "            gemma-2-9b-it@fireworks-ai                     gpt-4-turbo@openai  -0.8033  0.993  -2.5137  0.9071  False\n",
            "            gemma-2-9b-it@fireworks-ai                           gpt-4@openai  -1.1353 0.7448  -2.8457  0.5751  False\n",
            "            gemma-2-9b-it@fireworks-ai                          gpt-4o@openai  -0.8407 0.9872  -2.5511  0.8697  False\n",
            "            gemma-2-9b-it@fireworks-ai              llama-3-70b-chat@anyscale    2.114 0.9976  -2.7237  6.9517  False\n",
            "            gemma-2-9b-it@fireworks-ai          llama-3-70b-chat@fireworks-ai   0.4676    1.0  -1.2731  2.2082  False\n",
            "            gemma-2-9b-it@fireworks-ai               llama-3-8b-chat@anyscale   -0.326    1.0  -5.1637  4.5117  False\n",
            "            gemma-2-9b-it@fireworks-ai           llama-3-8b-chat@fireworks-ai  -0.1724    1.0  -1.9131  1.5682  False\n",
            "            gemma-2-9b-it@fireworks-ai      mistral-7b-instruct-v0.1@anyscale   -1.376    1.0  -6.2137  3.4617  False\n",
            "            gemma-2-9b-it@fireworks-ai   mistral-7b-instruct-v0.3@together-ai   -0.112    1.0  -1.8224  1.5984  False\n",
            "            gemma-2-9b-it@fireworks-ai              mistral-large@aws-bedrock  -0.3667    1.0  -2.0771  1.3437  False\n",
            "            gemma-2-9b-it@fireworks-ai               mistral-small@mistral-ai  -1.2333 0.5817  -2.9437  0.4771  False\n",
            "            gemma-2-9b-it@fireworks-ai   mixtral-8x22b-instruct-v0.1@anyscale   -0.676    1.0  -5.5137  4.1617  False\n",
            "            gemma-2-9b-it@fireworks-ai  mixtral-8x22b-instruct-v0.1@deepinfra   -0.781 0.9964  -2.5217  0.9597  False\n",
            "            gemma-2-9b-it@fireworks-ai    mixtral-8x7b-instruct-v0.1@anyscale   -1.086    1.0  -5.9237  3.7517  False\n",
            "            gemma-2-9b-it@fireworks-ai mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.7103 0.0615   -3.451  0.0304  False\n",
            "            gemma-2-9b-it@fireworks-ai                          original_text  -3.3373    0.0  -5.0477 -1.6269   True\n",
            "            gemma-2-9b-it@fireworks-ai          qwen-2-72b-instruct@deepinfra    0.109    1.0  -1.6317  1.8497  False\n",
            "               gemma-2b-it@together-ai                   gemma-7b-it@anyscale   1.9143 0.9995  -2.9342  6.7628  False\n",
            "               gemma-2b-it@together-ai                   gpt-3.5-turbo@openai   -0.453    1.0  -2.1937  1.2876  False\n",
            "               gemma-2b-it@together-ai                     gpt-4-turbo@openai   -0.593    1.0  -2.3337  1.1476  False\n",
            "               gemma-2b-it@together-ai                           gpt-4@openai   -0.925 0.9667  -2.6657  0.8156  False\n",
            "               gemma-2b-it@together-ai                          gpt-4o@openai  -0.6304 0.9999  -2.3711  1.1103  False\n",
            "               gemma-2b-it@together-ai              llama-3-70b-chat@anyscale   2.3243 0.9908  -2.5242  7.1728  False\n",
            "               gemma-2b-it@together-ai          llama-3-70b-chat@fireworks-ai   0.6779 0.9997  -1.0926  2.4483  False\n",
            "               gemma-2b-it@together-ai               llama-3-8b-chat@anyscale  -0.1157    1.0  -4.9642  4.7328  False\n",
            "               gemma-2b-it@together-ai           llama-3-8b-chat@fireworks-ai   0.0379    1.0  -1.7326  1.8083  False\n",
            "               gemma-2b-it@together-ai      mistral-7b-instruct-v0.1@anyscale  -1.1657    1.0  -6.0142  3.6828  False\n",
            "               gemma-2b-it@together-ai   mistral-7b-instruct-v0.3@together-ai   0.0983    1.0  -1.6424   1.839  False\n",
            "               gemma-2b-it@together-ai              mistral-large@aws-bedrock  -0.1564    1.0  -1.8971  1.5843  False\n",
            "               gemma-2b-it@together-ai               mistral-small@mistral-ai   -1.023 0.9048  -2.7637  0.7176  False\n",
            "               gemma-2b-it@together-ai   mixtral-8x22b-instruct-v0.1@anyscale  -0.4657    1.0  -5.3142  4.3828  False\n",
            "               gemma-2b-it@together-ai  mixtral-8x22b-instruct-v0.1@deepinfra  -0.5707    1.0  -2.3411  1.1997  False\n",
            "               gemma-2b-it@together-ai    mixtral-8x7b-instruct-v0.1@anyscale  -0.8757    1.0  -5.7242  3.9728  False\n",
            "               gemma-2b-it@together-ai mixtral-8x7b-instruct-v0.1@aws-bedrock     -1.5  0.244  -3.2704  0.2704  False\n",
            "               gemma-2b-it@together-ai                          original_text   -3.127    0.0  -4.8677 -1.3864   True\n",
            "               gemma-2b-it@together-ai          qwen-2-72b-instruct@deepinfra   0.3193    1.0  -1.4511  2.0897  False\n",
            "                  gemma-7b-it@anyscale                   gpt-3.5-turbo@openai  -2.3673 0.9879  -7.2051  2.4704  False\n",
            "                  gemma-7b-it@anyscale                     gpt-4-turbo@openai  -2.5073 0.9752  -7.3451  2.3304  False\n",
            "                  gemma-7b-it@anyscale                           gpt-4@openai  -2.8393  0.906  -7.6771  1.9984  False\n",
            "                  gemma-7b-it@anyscale                          gpt-4o@openai  -2.5447 0.9704  -7.3824  2.2931  False\n",
            "                  gemma-7b-it@anyscale              llama-3-70b-chat@anyscale     0.41    1.0  -6.2144  7.0344  False\n",
            "                  gemma-7b-it@anyscale          llama-3-70b-chat@fireworks-ai  -1.2364    1.0   -6.085  3.6121  False\n",
            "                  gemma-7b-it@anyscale               llama-3-8b-chat@anyscale    -2.03    1.0  -8.6544  4.5944  False\n",
            "                  gemma-7b-it@anyscale           llama-3-8b-chat@fireworks-ai  -1.8764 0.9997   -6.725  2.9721  False\n",
            "                  gemma-7b-it@anyscale      mistral-7b-instruct-v0.1@anyscale    -3.08  0.994  -9.7044  3.5444  False\n",
            "                  gemma-7b-it@anyscale   mistral-7b-instruct-v0.3@together-ai   -1.816 0.9998  -6.6537  3.0217  False\n",
            "                  gemma-7b-it@anyscale              mistral-large@aws-bedrock  -2.0707 0.9982  -6.9084  2.7671  False\n",
            "                  gemma-7b-it@anyscale               mistral-small@mistral-ai  -2.9373 0.8722  -7.7751  1.9004  False\n",
            "                  gemma-7b-it@anyscale   mixtral-8x22b-instruct-v0.1@anyscale    -2.38 0.9999  -9.0044  4.2444  False\n",
            "                  gemma-7b-it@anyscale  mixtral-8x22b-instruct-v0.1@deepinfra   -2.485 0.9783  -7.3335  2.3635  False\n",
            "                  gemma-7b-it@anyscale    mixtral-8x7b-instruct-v0.1@anyscale    -2.79 0.9986  -9.4144  3.8344  False\n",
            "                  gemma-7b-it@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -3.4143 0.6317  -8.2628  1.4342  False\n",
            "                  gemma-7b-it@anyscale                          original_text  -5.0413 0.0296  -9.8791 -0.2036   True\n",
            "                  gemma-7b-it@anyscale          qwen-2-72b-instruct@deepinfra   -1.595    1.0  -6.4435  3.2535  False\n",
            "                  gpt-3.5-turbo@openai                     gpt-4-turbo@openai    -0.14    1.0  -1.8504  1.5704  False\n",
            "                  gpt-3.5-turbo@openai                           gpt-4@openai   -0.472    1.0  -2.1824  1.2384  False\n",
            "                  gpt-3.5-turbo@openai                          gpt-4o@openai  -0.1773    1.0  -1.8877  1.5331  False\n",
            "                  gpt-3.5-turbo@openai              llama-3-70b-chat@anyscale   2.7773  0.924  -2.0604  7.6151  False\n",
            "                  gpt-3.5-turbo@openai          llama-3-70b-chat@fireworks-ai   1.1309 0.7806  -0.6098  2.8716  False\n",
            "                  gpt-3.5-turbo@openai               llama-3-8b-chat@anyscale   0.3373    1.0  -4.5004  5.1751  False\n",
            "                  gpt-3.5-turbo@openai           llama-3-8b-chat@fireworks-ai   0.4909    1.0  -1.2498  2.2316  False\n",
            "                  gpt-3.5-turbo@openai      mistral-7b-instruct-v0.1@anyscale  -0.7127    1.0  -5.5504  4.1251  False\n",
            "                  gpt-3.5-turbo@openai   mistral-7b-instruct-v0.3@together-ai   0.5513    1.0  -1.1591  2.2617  False\n",
            "                  gpt-3.5-turbo@openai              mistral-large@aws-bedrock   0.2967    1.0  -1.4137  2.0071  False\n",
            "                  gpt-3.5-turbo@openai               mistral-small@mistral-ai    -0.57    1.0  -2.2804  1.1404  False\n",
            "                  gpt-3.5-turbo@openai   mixtral-8x22b-instruct-v0.1@anyscale  -0.0127    1.0  -4.8504  4.8251  False\n",
            "                  gpt-3.5-turbo@openai  mixtral-8x22b-instruct-v0.1@deepinfra  -0.1177    1.0  -1.8583   1.623  False\n",
            "                  gpt-3.5-turbo@openai    mixtral-8x7b-instruct-v0.1@anyscale  -0.4227    1.0  -5.2604  4.4151  False\n",
            "                  gpt-3.5-turbo@openai mixtral-8x7b-instruct-v0.1@aws-bedrock   -1.047 0.8824  -2.7876  0.6937  False\n",
            "                  gpt-3.5-turbo@openai                          original_text   -2.674    0.0  -4.3844 -0.9636   True\n",
            "                  gpt-3.5-turbo@openai          qwen-2-72b-instruct@deepinfra   0.7723 0.9969  -0.9683   2.513  False\n",
            "                    gpt-4-turbo@openai                           gpt-4@openai   -0.332    1.0  -2.0424  1.3784  False\n",
            "                    gpt-4-turbo@openai                          gpt-4o@openai  -0.0373    1.0  -1.7477  1.6731  False\n",
            "                    gpt-4-turbo@openai              llama-3-70b-chat@anyscale   2.9173 0.8796  -1.9204  7.7551  False\n",
            "                    gpt-4-turbo@openai          llama-3-70b-chat@fireworks-ai   1.2709 0.5547  -0.4698  3.0116  False\n",
            "                    gpt-4-turbo@openai               llama-3-8b-chat@anyscale   0.4773    1.0  -4.3604  5.3151  False\n",
            "                    gpt-4-turbo@openai           llama-3-8b-chat@fireworks-ai   0.6309 0.9999  -1.1098  2.3716  False\n",
            "                    gpt-4-turbo@openai      mistral-7b-instruct-v0.1@anyscale  -0.5727    1.0  -5.4104  4.2651  False\n",
            "                    gpt-4-turbo@openai   mistral-7b-instruct-v0.3@together-ai   0.6913 0.9993  -1.0191  2.4017  False\n",
            "                    gpt-4-turbo@openai              mistral-large@aws-bedrock   0.4367    1.0  -1.2737  2.1471  False\n",
            "                    gpt-4-turbo@openai               mistral-small@mistral-ai    -0.43    1.0  -2.1404  1.2804  False\n",
            "                    gpt-4-turbo@openai   mixtral-8x22b-instruct-v0.1@anyscale   0.1273    1.0  -4.7104  4.9651  False\n",
            "                    gpt-4-turbo@openai  mixtral-8x22b-instruct-v0.1@deepinfra   0.0223    1.0  -1.7183   1.763  False\n",
            "                    gpt-4-turbo@openai    mixtral-8x7b-instruct-v0.1@anyscale  -0.2827    1.0  -5.1204  4.5551  False\n",
            "                    gpt-4-turbo@openai mixtral-8x7b-instruct-v0.1@aws-bedrock   -0.907 0.9736  -2.6476  0.8337  False\n",
            "                    gpt-4-turbo@openai                          original_text   -2.534    0.0  -4.2444 -0.8236   True\n",
            "                    gpt-4-turbo@openai          qwen-2-72b-instruct@deepinfra   0.9123 0.9716  -0.8283   2.653  False\n",
            "                          gpt-4@openai                          gpt-4o@openai   0.2947    1.0  -1.4157  2.0051  False\n",
            "                          gpt-4@openai              llama-3-70b-chat@anyscale   3.2493 0.7238  -1.5884  8.0871  False\n",
            "                          gpt-4@openai          llama-3-70b-chat@fireworks-ai   1.6029 0.1214  -0.1378  3.3436  False\n",
            "                          gpt-4@openai               llama-3-8b-chat@anyscale   0.8093    1.0  -4.0284  5.6471  False\n",
            "                          gpt-4@openai           llama-3-8b-chat@fireworks-ai   0.9629 0.9481  -0.7778  2.7036  False\n",
            "                          gpt-4@openai      mistral-7b-instruct-v0.1@anyscale  -0.2407    1.0  -5.0784  4.5971  False\n",
            "                          gpt-4@openai   mistral-7b-instruct-v0.3@together-ai   1.0233 0.8878  -0.6871  2.7337  False\n",
            "                          gpt-4@openai              mistral-large@aws-bedrock   0.7687 0.9963  -0.9417  2.4791  False\n",
            "                          gpt-4@openai               mistral-small@mistral-ai   -0.098    1.0  -1.8084  1.6124  False\n",
            "                          gpt-4@openai   mixtral-8x22b-instruct-v0.1@anyscale   0.4593    1.0  -4.3784  5.2971  False\n",
            "                          gpt-4@openai  mixtral-8x22b-instruct-v0.1@deepinfra   0.3543    1.0  -1.3863   2.095  False\n",
            "                          gpt-4@openai    mixtral-8x7b-instruct-v0.1@anyscale   0.0493    1.0  -4.7884  4.8871  False\n",
            "                          gpt-4@openai mixtral-8x7b-instruct-v0.1@aws-bedrock   -0.575    1.0  -2.3156  1.1657  False\n",
            "                          gpt-4@openai                          original_text   -2.202 0.0008  -3.9124 -0.4916   True\n",
            "                          gpt-4@openai          qwen-2-72b-instruct@deepinfra   1.2443 0.6002  -0.4963   2.985  False\n",
            "                         gpt-4o@openai              llama-3-70b-chat@anyscale   2.9547 0.8655  -1.8831  7.7924  False\n",
            "                         gpt-4o@openai          llama-3-70b-chat@fireworks-ai   1.3082  0.491  -0.4324  3.0489  False\n",
            "                         gpt-4o@openai               llama-3-8b-chat@anyscale   0.5147    1.0  -4.3231  5.3524  False\n",
            "                         gpt-4o@openai           llama-3-8b-chat@fireworks-ai   0.6682 0.9997  -1.0724  2.4089  False\n",
            "                         gpt-4o@openai      mistral-7b-instruct-v0.1@anyscale  -0.5353    1.0  -5.3731  4.3024  False\n",
            "                         gpt-4o@openai   mistral-7b-instruct-v0.3@together-ai   0.7287 0.9983  -0.9817  2.4391  False\n",
            "                         gpt-4o@openai              mistral-large@aws-bedrock    0.474    1.0  -1.2364  2.1844  False\n",
            "                         gpt-4o@openai               mistral-small@mistral-ai  -0.3927    1.0  -2.1031  1.3177  False\n",
            "                         gpt-4o@openai   mixtral-8x22b-instruct-v0.1@anyscale   0.1647    1.0  -4.6731  5.0024  False\n",
            "                         gpt-4o@openai  mixtral-8x22b-instruct-v0.1@deepinfra   0.0597    1.0   -1.681  1.8003  False\n",
            "                         gpt-4o@openai    mixtral-8x7b-instruct-v0.1@anyscale  -0.2453    1.0  -5.0831  4.5924  False\n",
            "                         gpt-4o@openai mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.8696 0.9842  -2.6103  0.8711  False\n",
            "                         gpt-4o@openai                          original_text  -2.4967    0.0  -4.2071 -0.7863   True\n",
            "                         gpt-4o@openai          qwen-2-72b-instruct@deepinfra   0.9497 0.9553   -0.791  2.6903  False\n",
            "             llama-3-70b-chat@anyscale          llama-3-70b-chat@fireworks-ai  -1.6464    1.0   -6.495  3.2021  False\n",
            "             llama-3-70b-chat@anyscale               llama-3-8b-chat@anyscale    -2.44 0.9999  -9.0644  4.1844  False\n",
            "             llama-3-70b-chat@anyscale           llama-3-8b-chat@fireworks-ai  -2.2864 0.9926   -7.135  2.5621  False\n",
            "             llama-3-70b-chat@anyscale      mistral-7b-instruct-v0.1@anyscale    -3.49 0.9699 -10.1144  3.1344  False\n",
            "             llama-3-70b-chat@anyscale   mistral-7b-instruct-v0.3@together-ai   -2.226 0.9948  -7.0637  2.6117  False\n",
            "             llama-3-70b-chat@anyscale              mistral-large@aws-bedrock  -2.4807 0.9782  -7.3184  2.3571  False\n",
            "             llama-3-70b-chat@anyscale               mistral-small@mistral-ai  -3.3473 0.6673  -8.1851  1.4904  False\n",
            "             llama-3-70b-chat@anyscale   mixtral-8x22b-instruct-v0.1@anyscale    -2.79 0.9986  -9.4144  3.8344  False\n",
            "             llama-3-70b-chat@anyscale  mixtral-8x22b-instruct-v0.1@deepinfra   -2.895 0.8898  -7.7435  1.9535  False\n",
            "             llama-3-70b-chat@anyscale    mixtral-8x7b-instruct-v0.1@anyscale     -3.2 0.9898  -9.8244  3.4244  False\n",
            "             llama-3-70b-chat@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -3.8243 0.3854  -8.6728  1.0242  False\n",
            "             llama-3-70b-chat@anyscale                          original_text  -5.4513 0.0094 -10.2891 -0.6136   True\n",
            "             llama-3-70b-chat@anyscale          qwen-2-72b-instruct@deepinfra   -2.005  0.999  -6.8535  2.8435  False\n",
            "         llama-3-70b-chat@fireworks-ai               llama-3-8b-chat@anyscale  -0.7936    1.0  -5.6421   4.055  False\n",
            "         llama-3-70b-chat@fireworks-ai           llama-3-8b-chat@fireworks-ai    -0.64 0.9999  -2.4104  1.1304  False\n",
            "         llama-3-70b-chat@fireworks-ai      mistral-7b-instruct-v0.1@anyscale  -1.8436 0.9997  -6.6921   3.005  False\n",
            "         llama-3-70b-chat@fireworks-ai   mistral-7b-instruct-v0.3@together-ai  -0.5796    1.0  -2.3202  1.1611  False\n",
            "         llama-3-70b-chat@fireworks-ai              mistral-large@aws-bedrock  -0.8342 0.9908  -2.5749  0.9064  False\n",
            "         llama-3-70b-chat@fireworks-ai               mistral-small@mistral-ai  -1.7009 0.0655  -3.4416  0.0398  False\n",
            "         llama-3-70b-chat@fireworks-ai   mixtral-8x22b-instruct-v0.1@anyscale  -1.1436    1.0  -5.9921   3.705  False\n",
            "         llama-3-70b-chat@fireworks-ai  mixtral-8x22b-instruct-v0.1@deepinfra  -1.2486 0.6286   -3.019  0.5219  False\n",
            "         llama-3-70b-chat@fireworks-ai    mixtral-8x7b-instruct-v0.1@anyscale  -1.5536    1.0  -6.4021   3.295  False\n",
            "         llama-3-70b-chat@fireworks-ai mixtral-8x7b-instruct-v0.1@aws-bedrock  -2.1779  0.002  -3.9483 -0.4074   True\n",
            "         llama-3-70b-chat@fireworks-ai                          original_text  -3.8049    0.0  -5.5456 -2.0642   True\n",
            "         llama-3-70b-chat@fireworks-ai          qwen-2-72b-instruct@deepinfra  -0.3586    1.0   -2.129  1.4119  False\n",
            "              llama-3-8b-chat@anyscale           llama-3-8b-chat@fireworks-ai   0.1536    1.0   -4.695  5.0021  False\n",
            "              llama-3-8b-chat@anyscale      mistral-7b-instruct-v0.1@anyscale    -1.05    1.0  -7.6744  5.5744  False\n",
            "              llama-3-8b-chat@anyscale   mistral-7b-instruct-v0.3@together-ai    0.214    1.0  -4.6237  5.0517  False\n",
            "              llama-3-8b-chat@anyscale              mistral-large@aws-bedrock  -0.0407    1.0  -4.8784  4.7971  False\n",
            "              llama-3-8b-chat@anyscale               mistral-small@mistral-ai  -0.9073    1.0  -5.7451  3.9304  False\n",
            "              llama-3-8b-chat@anyscale   mixtral-8x22b-instruct-v0.1@anyscale    -0.35    1.0  -6.9744  6.2744  False\n",
            "              llama-3-8b-chat@anyscale  mixtral-8x22b-instruct-v0.1@deepinfra   -0.455    1.0  -5.3035  4.3935  False\n",
            "              llama-3-8b-chat@anyscale    mixtral-8x7b-instruct-v0.1@anyscale    -0.76    1.0  -7.3844  5.8644  False\n",
            "              llama-3-8b-chat@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.3843    1.0  -6.2328  3.4642  False\n",
            "              llama-3-8b-chat@anyscale                          original_text  -3.0113 0.8424  -7.8491  1.8264  False\n",
            "              llama-3-8b-chat@anyscale          qwen-2-72b-instruct@deepinfra    0.435    1.0  -4.4135  5.2835  False\n",
            "          llama-3-8b-chat@fireworks-ai      mistral-7b-instruct-v0.1@anyscale  -1.2036    1.0  -6.0521   3.645  False\n",
            "          llama-3-8b-chat@fireworks-ai   mistral-7b-instruct-v0.3@together-ai   0.0604    1.0  -1.6802  1.8011  False\n",
            "          llama-3-8b-chat@fireworks-ai              mistral-large@aws-bedrock  -0.1942    1.0  -1.9349  1.5464  False\n",
            "          llama-3-8b-chat@fireworks-ai               mistral-small@mistral-ai  -1.0609 0.8679  -2.8016  0.6798  False\n",
            "          llama-3-8b-chat@fireworks-ai   mixtral-8x22b-instruct-v0.1@anyscale  -0.5036    1.0  -5.3521   4.345  False\n",
            "          llama-3-8b-chat@fireworks-ai  mixtral-8x22b-instruct-v0.1@deepinfra  -0.6086    1.0   -2.379  1.1619  False\n",
            "          llama-3-8b-chat@fireworks-ai    mixtral-8x7b-instruct-v0.1@anyscale  -0.9136    1.0  -5.7621   3.935  False\n",
            "          llama-3-8b-chat@fireworks-ai mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.5379 0.2019  -3.3083  0.2326  False\n",
            "          llama-3-8b-chat@fireworks-ai                          original_text  -3.1649    0.0  -4.9056 -1.4242   True\n",
            "          llama-3-8b-chat@fireworks-ai          qwen-2-72b-instruct@deepinfra   0.2814    1.0   -1.489  2.0519  False\n",
            "     mistral-7b-instruct-v0.1@anyscale   mistral-7b-instruct-v0.3@together-ai    1.264    1.0  -3.5737  6.1017  False\n",
            "     mistral-7b-instruct-v0.1@anyscale              mistral-large@aws-bedrock   1.0093    1.0  -3.8284  5.8471  False\n",
            "     mistral-7b-instruct-v0.1@anyscale               mistral-small@mistral-ai   0.1427    1.0  -4.6951  4.9804  False\n",
            "     mistral-7b-instruct-v0.1@anyscale   mixtral-8x22b-instruct-v0.1@anyscale      0.7    1.0  -5.9244  7.3244  False\n",
            "     mistral-7b-instruct-v0.1@anyscale  mixtral-8x22b-instruct-v0.1@deepinfra    0.595    1.0  -4.2535  5.4435  False\n",
            "     mistral-7b-instruct-v0.1@anyscale    mixtral-8x7b-instruct-v0.1@anyscale     0.29    1.0  -6.3344  6.9144  False\n",
            "     mistral-7b-instruct-v0.1@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.3343    1.0  -5.1828  4.5142  False\n",
            "     mistral-7b-instruct-v0.1@anyscale                          original_text  -1.9613 0.9993  -6.7991  2.8764  False\n",
            "     mistral-7b-instruct-v0.1@anyscale          qwen-2-72b-instruct@deepinfra    1.485    1.0  -3.3635  6.3335  False\n",
            "  mistral-7b-instruct-v0.3@together-ai              mistral-large@aws-bedrock  -0.2547    1.0  -1.9651  1.4557  False\n",
            "  mistral-7b-instruct-v0.3@together-ai               mistral-small@mistral-ai  -1.1213 0.7659  -2.8317  0.5891  False\n",
            "  mistral-7b-instruct-v0.3@together-ai   mixtral-8x22b-instruct-v0.1@anyscale   -0.564    1.0  -5.4017  4.2737  False\n",
            "  mistral-7b-instruct-v0.3@together-ai  mixtral-8x22b-instruct-v0.1@deepinfra   -0.669 0.9997  -2.4097  1.0717  False\n",
            "  mistral-7b-instruct-v0.3@together-ai    mixtral-8x7b-instruct-v0.1@anyscale   -0.974    1.0  -5.8117  3.8637  False\n",
            "  mistral-7b-instruct-v0.3@together-ai mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.5983 0.1248   -3.339  0.1424  False\n",
            "  mistral-7b-instruct-v0.3@together-ai                          original_text  -3.2253    0.0  -4.9357 -1.5149   True\n",
            "  mistral-7b-instruct-v0.3@together-ai          qwen-2-72b-instruct@deepinfra    0.221    1.0  -1.5197  1.9617  False\n",
            "             mistral-large@aws-bedrock               mistral-small@mistral-ai  -0.8667 0.9811  -2.5771  0.8437  False\n",
            "             mistral-large@aws-bedrock   mixtral-8x22b-instruct-v0.1@anyscale  -0.3093    1.0  -5.1471  4.5284  False\n",
            "             mistral-large@aws-bedrock  mixtral-8x22b-instruct-v0.1@deepinfra  -0.4143    1.0   -2.155  1.3263  False\n",
            "             mistral-large@aws-bedrock    mixtral-8x7b-instruct-v0.1@anyscale  -0.7193    1.0  -5.5571  4.1184  False\n",
            "             mistral-large@aws-bedrock mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.3436 0.4322  -3.0843  0.3971  False\n",
            "             mistral-large@aws-bedrock                          original_text  -2.9707    0.0  -4.6811 -1.2603   True\n",
            "             mistral-large@aws-bedrock          qwen-2-72b-instruct@deepinfra   0.4757    1.0   -1.265  2.2163  False\n",
            "              mistral-small@mistral-ai   mixtral-8x22b-instruct-v0.1@anyscale   0.5573    1.0  -4.2804  5.3951  False\n",
            "              mistral-small@mistral-ai  mixtral-8x22b-instruct-v0.1@deepinfra   0.4523    1.0  -1.2883   2.193  False\n",
            "              mistral-small@mistral-ai    mixtral-8x7b-instruct-v0.1@anyscale   0.1473    1.0  -4.6904  4.9851  False\n",
            "              mistral-small@mistral-ai mixtral-8x7b-instruct-v0.1@aws-bedrock   -0.477    1.0  -2.2176  1.2637  False\n",
            "              mistral-small@mistral-ai                          original_text   -2.104  0.002  -3.8144 -0.3936   True\n",
            "              mistral-small@mistral-ai          qwen-2-72b-instruct@deepinfra   1.3423 0.4343  -0.3983   3.083  False\n",
            "  mixtral-8x22b-instruct-v0.1@anyscale  mixtral-8x22b-instruct-v0.1@deepinfra   -0.105    1.0  -4.9535  4.7435  False\n",
            "  mixtral-8x22b-instruct-v0.1@anyscale    mixtral-8x7b-instruct-v0.1@anyscale    -0.41    1.0  -7.0344  6.2144  False\n",
            "  mixtral-8x22b-instruct-v0.1@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -1.0343    1.0  -5.8828  3.8142  False\n",
            "  mixtral-8x22b-instruct-v0.1@anyscale                          original_text  -2.6613 0.9511  -7.4991  2.1764  False\n",
            "  mixtral-8x22b-instruct-v0.1@anyscale          qwen-2-72b-instruct@deepinfra    0.785    1.0  -4.0635  5.6335  False\n",
            " mixtral-8x22b-instruct-v0.1@deepinfra    mixtral-8x7b-instruct-v0.1@anyscale   -0.305    1.0  -5.1535  4.5435  False\n",
            " mixtral-8x22b-instruct-v0.1@deepinfra mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.9293 0.9711  -2.6997  0.8411  False\n",
            " mixtral-8x22b-instruct-v0.1@deepinfra                          original_text  -2.5563    0.0   -4.297 -0.8157   True\n",
            " mixtral-8x22b-instruct-v0.1@deepinfra          qwen-2-72b-instruct@deepinfra     0.89 0.9829  -0.8804  2.6604  False\n",
            "   mixtral-8x7b-instruct-v0.1@anyscale mixtral-8x7b-instruct-v0.1@aws-bedrock  -0.6243    1.0  -5.4728  4.2242  False\n",
            "   mixtral-8x7b-instruct-v0.1@anyscale                          original_text  -2.2513 0.9939  -7.0891  2.5864  False\n",
            "   mixtral-8x7b-instruct-v0.1@anyscale          qwen-2-72b-instruct@deepinfra    1.195    1.0  -3.6535  6.0435  False\n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock                          original_text   -1.627 0.1049  -3.3677  0.1136  False\n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock          qwen-2-72b-instruct@deepinfra   1.8193 0.0356   0.0489  3.5897   True\n",
            "                         original_text          qwen-2-72b-instruct@deepinfra   3.4463    0.0   1.7057   5.187   True\n",
            "---------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_table = filtered_df.groupby('model')['Coleman-Liau Index'].describe()\n",
        "anova_df = pd.DataFrame(anova_table)\n",
        "output_file_path = 'summary_and_anova_output_Coleman-Liau Index.xlsx'\n",
        "\n",
        "with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:\n",
        "\n",
        "    summary_table.to_excel(writer, sheet_name='Summary Statistics')\n",
        "    anova_df.to_excel(writer, sheet_name='ANOVA Table')\n",
        "\n",
        "print(f\"Data has been saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxU-pO81iXSB",
        "outputId": "0d357cba-fd83-4ee9-e130-54e2e89c6c1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to summary_and_anova_output_Coleman-Liau Index.xlsx\n"
          ]
        }
      ]
    }
  ]
}