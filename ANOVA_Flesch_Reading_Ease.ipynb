{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.stats.multicomp as mc\n",
        "\n",
        "df_new = pd.read_excel('overalldata.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "exclude_models = [\n",
        "    \"codellama-70b-instruct@anyscale\",\n",
        "    \"codellama-7b-instruct@together-ai\",\n",
        "    \"command-r-plus@aws-bedrock\",\n",
        "    \"deepseek-coder-33b-instruct@together-ai\",\n",
        "    \"phi-3-medium-4k-instruct@deepinfra\"\n",
        "]\n",
        "\n",
        "filtered_df = df_new[~df_new['model'].isin(exclude_models)]\n",
        "\n",
        "control_group = filtered_df[filtered_df['model'] == 'original_text']\n",
        "other_groups = filtered_df[filtered_df['model'] != 'original_text']\n",
        "\n",
        "combined_df = pd.concat([control_group, other_groups])\n",
        "\n",
        "#  one-way ANOVA\n",
        "model = ols('Q(\"Flesch Reading Ease\") ~ C(model)', data=combined_df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "print(anova_table)\n",
        "\n",
        "# post-hoc analysis using Tukey's HSD test\n",
        "comp = mc.pairwise_tukeyhsd(combined_df['Flesch Reading Ease'], combined_df['model'])\n",
        "\n",
        "tukey_results = pd.DataFrame(data=comp.summary().data[1:], columns=comp.summary().data[0])\n",
        "\n",
        "# Filter for comparisons with the control group 'original_text'\n",
        "control_comparisons = tukey_results[tukey_results['group1'] == 'original_text']\n",
        "\n",
        "# Sort by absolute mean difference to find the most similar models\n",
        "control_comparisons_sorted = control_comparisons.sort_values(by='meandiff', key=abs)\n",
        "\n",
        "print(control_comparisons_sorted.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6VkMr4jntoy",
        "outputId": "961f69eb-788d-44ce-b6ad-a431f7f9b600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                sum_sq     df         F        PR(>F)\n",
            "C(model)   4398.763496   28.0  3.762974  6.159026e-09\n",
            "Residual  11439.102644  274.0       NaN           NaN\n",
            "            group1                         group2  meandiff  p-adj    lower  \\\n",
            "405  original_text  qwen-2-72b-instruct@deepinfra   -16.121    0.0 -25.1787   \n",
            "\n",
            "      upper  reject  \n",
            "405 -7.0632    True  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "df_new = pd.read_excel('overalldata.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "exclude_models = [\n",
        "    \"codellama-70b-instruct@anyscale\",\n",
        "    \"codellama-7b-instruct@together-ai\",\n",
        "    \"command-r-plus@aws-bedrock\",\n",
        "    \"deepseek-coder-33b-instruct@together-ai\",\n",
        "    \"phi-3-medium-4k-instruct@deepinfra\",\n",
        "    \"nemotron-4-340b-instruct@deepinfra\",\n",
        "    \"codellama-13b-instruct@together-ai\"\n",
        "]\n",
        "\n",
        "filtered_df = df_new[~df_new['model'].isin(exclude_models)]\n",
        "\n",
        "#  a summary table for the 'Flesch Reading Ease' grouped by 'model'\n",
        "summary_table = filtered_df.groupby('model')['Flesch Reading Ease'].describe()\n",
        "\n",
        "print(summary_table)\n",
        "\n",
        "# Define and fit the ANOVA model\n",
        "model = ols('Q(\"Flesch Reading Ease\") ~ C(model)', data=filtered_df).fit()\n",
        "\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "print(anova_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWQpOogXsdbp",
        "outputId": "eb8cac94-56e0-4545-92f1-a6632ce5d876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        count       mean       std    min  \\\n",
            "model                                                                       \n",
            "claude-3-haiku@anthropic                 15.0  77.174667  7.697056  63.09   \n",
            "claude-3-opus@anthropic                  15.0  75.615333  8.322188  56.79   \n",
            "claude-3-sonnet@anthropic                 1.0  66.670000       NaN  66.67   \n",
            "claude-3.5-sonnet@anthropic              15.0  78.171333  7.014159  69.92   \n",
            "gemini-1.5-flash@vertex-ai               15.0  77.988667  6.291568  68.60   \n",
            "gemini-1.5-pro@vertex-ai                 15.0  77.161333  4.992453  70.43   \n",
            "gemma-2-9b-it@fireworks-ai               15.0  78.304667  7.477700  62.27   \n",
            "gemma-2b-it@together-ai                  14.0  74.838571  7.094929  63.19   \n",
            "gemma-7b-it@anyscale                      1.0  64.000000       NaN  64.00   \n",
            "gpt-3.5-turbo@openai                     15.0  74.166000  6.298815  59.33   \n",
            "gpt-4-turbo@openai                       15.0  79.054667  4.369576  70.94   \n",
            "gpt-4@openai                             15.0  78.376000  4.801910  71.75   \n",
            "gpt-4o@openai                            15.0  78.514667  4.662498  71.44   \n",
            "llama-3-70b-chat@anyscale                 1.0  60.040000       NaN  60.04   \n",
            "llama-3-70b-chat@fireworks-ai            14.0  72.223571  6.043796  61.26   \n",
            "llama-3-8b-chat@anyscale                  1.0  70.530000       NaN  70.53   \n",
            "llama-3-8b-chat@fireworks-ai             14.0  73.249286  8.067601  58.21   \n",
            "mistral-7b-instruct-v0.1@anyscale         1.0  70.430000       NaN  70.43   \n",
            "mistral-7b-instruct-v0.3@together-ai     15.0  74.281333  6.300849  65.76   \n",
            "mistral-large@aws-bedrock                15.0  74.727333  4.821155  67.18   \n",
            "mistral-small@mistral-ai                 15.0  78.455333  7.085910  63.39   \n",
            "mixtral-8x22b-instruct-v0.1@anyscale      1.0  77.270000       NaN  77.27   \n",
            "mixtral-8x22b-instruct-v0.1@deepinfra    14.0  78.064286  4.778522  70.33   \n",
            "mixtral-8x7b-instruct-v0.1@anyscale       1.0  71.040000       NaN  71.04   \n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock   14.0  81.270714  6.904666  69.92   \n",
            "original_text                            15.0  87.556667  8.531143  65.83   \n",
            "qwen-2-72b-instruct@deepinfra            14.0  71.435714  4.964072  65.05   \n",
            "\n",
            "                                            25%     50%      75%    max  \n",
            "model                                                                    \n",
            "claude-3-haiku@anthropic                69.9200  79.400  82.0350  88.06  \n",
            "claude-3-opus@anthropic                 70.6350  76.110  80.4600  90.80  \n",
            "claude-3-sonnet@anthropic               66.6700  66.670  66.6700  66.67  \n",
            "claude-3.5-sonnet@anthropic             72.5600  74.490  81.8850  93.03  \n",
            "gemini-1.5-flash@vertex-ai              72.9150  77.030  82.2900  91.00  \n",
            "gemini-1.5-pro@vertex-ai                73.6300  75.810  83.1000  84.47  \n",
            "gemma-2-9b-it@fireworks-ai              73.5750  79.300  83.1000  93.14  \n",
            "gemma-2b-it@together-ai                 68.9100  74.150  82.0625  83.66  \n",
            "gemma-7b-it@anyscale                    64.0000  64.000  64.0000  64.00  \n",
            "gpt-3.5-turbo@openai                    71.0350  74.900  78.6850  81.87  \n",
            "gpt-4-turbo@openai                      76.8600  80.110  82.1900  85.28  \n",
            "gpt-4@openai                            73.7800  80.920  81.8350  85.39  \n",
            "gpt-4o@openai                           75.1000  78.690  82.5950  84.68  \n",
            "llama-3-70b-chat@anyscale               60.0400  60.040  60.0400  60.04  \n",
            "llama-3-70b-chat@fireworks-ai           69.2375  71.140  77.6100  82.04  \n",
            "llama-3-8b-chat@anyscale                70.5300  70.530  70.5300  70.53  \n",
            "llama-3-8b-chat@fireworks-ai            68.7075  72.055  80.5425  85.79  \n",
            "mistral-7b-instruct-v0.1@anyscale       70.4300  70.430  70.4300  70.43  \n",
            "mistral-7b-instruct-v0.3@together-ai    69.6150  71.340  78.6850  88.16  \n",
            "mistral-large@aws-bedrock               70.6300  74.900  78.4350  82.85  \n",
            "mistral-small@mistral-ai                74.3400  80.620  82.0850  90.50  \n",
            "mixtral-8x22b-instruct-v0.1@anyscale    77.2700  77.270  77.2700  77.27  \n",
            "mixtral-8x22b-instruct-v0.1@deepinfra   74.6925  78.635  81.9350  84.57  \n",
            "mixtral-8x7b-instruct-v0.1@anyscale     71.0400  71.040  71.0400  71.04  \n",
            "mixtral-8x7b-instruct-v0.1@aws-bedrock  76.9150  80.765  85.1175  93.44  \n",
            "original_text                           82.8000  88.060  94.1650  99.46  \n",
            "qwen-2-72b-instruct@deepinfra           67.3575  70.180  76.4325  80.41  \n",
            "                sum_sq     df         F        PR(>F)\n",
            "C(model)   4194.178990   26.0  3.863956  7.454729e-09\n",
            "Residual  11439.102644  274.0       NaN           NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas xlsxwriter\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GypFI0DKiGdR",
        "outputId": "896872a8-4dbd-4a44-a82b-a01d5a14ea63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "summary_table = filtered_df.groupby('model')['Flesch Reading Ease'].describe()\n",
        "anova_df = pd.DataFrame(anova_table)\n",
        "\n",
        "output_file_path = 'summary_and_anova_output_Flesch Reading Ease.xlsx'\n",
        "with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:\n",
        "    summary_table.to_excel(writer, sheet_name='Summary Statistics')\n",
        "    anova_df.to_excel(writer, sheet_name='ANOVA Table')\n",
        "\n",
        "print(f\"Data has been saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2hnUKfuiJAH",
        "outputId": "9c43a5e7-ba56-45bc-de45-1f37d7a91fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to summary_and_anova_output_Flesch Reading Ease.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "anova_df = pd.DataFrame(anova_table)\n",
        "output_file = 'anova_table_output_Flesch Reading Ease.xlsx'\n",
        "anova_df.to_excel(output_file, index=True)\n",
        "\n",
        "print(f\"ANOVA table saved as {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPxaVp0zhlxm",
        "outputId": "4b5f806d-9a33-4e98-ce66-82c30dd6a6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANOVA table saved as anova_table_output_Flesch Reading Ease.xlsx\n"
          ]
        }
      ]
    }
  ]
}